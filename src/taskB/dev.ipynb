{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/rah_python312_cuda124/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Term-to-Type RAG Implementation\n",
    "\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from typing import List, Dict, Any\n",
    "import shutil\n",
    "\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ó–∞–≥—Ä—É–∑–∫–∞ train –¥–∞–Ω–Ω—ã—Ö...\n",
      "MatOnto train: 85 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
      "OBI train: 201 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
      "SWEET train: 1558 –ø—Ä–∏–º–µ—Ä–æ–≤\n",
      "\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ test –¥–∞–Ω–Ω—ã—Ö...\n",
      "MatOnto test: 37 —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
      "OBI test: 87 —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
      "SWEET test: 626 —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
      "\n",
      "‚úÖ –î–∞–Ω–Ω—ã–µ TaskB –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\n",
      "Train –¥–æ–º–µ–Ω–æ–≤: ['MatOnto', 'OBI', 'SWEET']\n",
      "Test –¥–æ–º–µ–Ω–æ–≤: ['MatOnto', 'OBI', 'SWEET']\n",
      "\n",
      "üìä –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:\n",
      "\n",
      "MatOnto train –ø—Ä–∏–º–µ—Ä:\n",
      "{\n",
      "  \"id\": \"TT_778fb090\",\n",
      "  \"term\": \"kilogram\",\n",
      "  \"types\": [\n",
      "    \"mass unit\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "MatOnto test –ø—Ä–∏–º–µ—Ä:\n",
      "{\n",
      "  \"term\": \"newton\",\n",
      "  \"types\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def load_terms2types_data(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Loads terms2types data from a JSON file\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_taskb_test_data(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Loads TaskB test data from a JSON file (not txt!)\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Valid domains for TaskB\n",
    "domains = ['MatOnto', 'OBI', 'SWEET']\n",
    "\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "for domain in domains:\n",
    "    train_path = f'/home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/2025/TaskB-TermTyping/{domain}/train/term_typing_train_data.json'\n",
    "    train_data[domain] = load_terms2types_data(train_path)\n",
    "    print(f\"{domain} train: {len(train_data[domain])} examples\")\n",
    "\n",
    "print(\"\\nLoading test data...\")\n",
    "for domain in domains:\n",
    "    # FIXED: using lowercase file names\n",
    "    domain_lower = domain.lower()\n",
    "    test_path = f'../../../2025/TaskB-TermTyping/{domain}/test/{domain_lower}_term_typing_test_data.json'\n",
    "    \n",
    "    if os.path.exists(test_path):\n",
    "        test_json_data = load_taskb_test_data(test_path)\n",
    "        # Convert to required format\n",
    "        test_data[domain] = [{\"term\": item[\"term\"], \"types\": []} for item in test_json_data]\n",
    "        print(f\"{domain} test: {len(test_data[domain])} terms\")\n",
    "    else:\n",
    "        print(f\"‚ùå Test file not found: {test_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ TaskB data loaded successfully!\")\n",
    "print(f\"Train domains: {list(train_data.keys())}\")\n",
    "print(f\"Test domains: {list(test_data.keys())}\")\n",
    "\n",
    "# Show data examples\n",
    "print(\"\\nüìä Data examples:\")\n",
    "for domain in domains:\n",
    "    if domain in train_data and train_data[domain]:\n",
    "        print(f\"\\n{domain} train example:\")\n",
    "        print(json.dumps(train_data[domain][0], indent=2, ensure_ascii=False))\n",
    "        break\n",
    "\n",
    "for domain in domains:\n",
    "    if domain in test_data and test_data[domain]:\n",
    "        print(f\"\\n{domain} test example:\")\n",
    "        print(json.dumps(test_data[domain][0], indent=2, ensure_ascii=False))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 27.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!\n"
     ]
    }
   ],
   "source": [
    "# 2. Embedding Model Initialization (Qwen3-Embedding)\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    \"\"\"Pooling function to obtain embeddings\"\"\"\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    \"\"\"Creates an instruction prompt for the embedding model\"\"\"\n",
    "    return f'Instruct: {task_description}\\nQuery: {query}'\n",
    "\n",
    "print(\"Initializing the embedding model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-4B', padding_side='left')\n",
    "model = AutoModel.from_pretrained(\n",
    "    'Qwen/Qwen3-Embedding-4B', \n",
    "    attn_implementation=\"flash_attention_2\", \n",
    "    torch_dtype=torch.bfloat16\n",
    ").cuda()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –≥–æ—Ç–æ–≤—ã!\n"
     ]
    }
   ],
   "source": [
    "# 3. Functions for Generating Embeddings\n",
    "\n",
    "def get_embeddings_batch(texts: List[str], model, tokenizer, max_length=8192, batch_size=8):\n",
    "    \"\"\"Generate embeddings for texts in batches\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        batch_tokenized = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        batch_tokenized = {k: v.to(model.device) for k, v in batch_tokenized.items()}\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch_tokenized)\n",
    "            batch_embeddings = last_token_pool(outputs.last_hidden_state, batch_tokenized['attention_mask'])\n",
    "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1)\n",
    "            all_embeddings.append(batch_embeddings.cpu())\n",
    "        \n",
    "        # Memory cleanup\n",
    "        del batch_tokenized, outputs, batch_embeddings\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "def compute_similarity_matrix(embeddings1, embeddings2, batch_size=100):\n",
    "    \"\"\"Compute a similarity matrix between two embedding sets in batches\"\"\"\n",
    "    n1, n2 = embeddings1.shape[0], embeddings2.shape[0]\n",
    "    similarity_matrix = torch.zeros(n1, n2)\n",
    "    \n",
    "    for i in tqdm(range(0, n1, batch_size), desc=\"Computing similarity\"):\n",
    "        end_i = min(i + batch_size, n1)\n",
    "        batch1 = embeddings1[i:end_i]\n",
    "        \n",
    "        for j in range(0, n2, batch_size):\n",
    "            end_j = min(j + batch_size, n2)\n",
    "            batch2 = embeddings2[j:end_j]\n",
    "            \n",
    "            # Compute similarity for the batch\n",
    "            similarity_batch = torch.mm(batch1, batch2.T)\n",
    "            similarity_matrix[i:end_i, j:end_j] = similarity_batch\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "print(\"Embedding utility functions are ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–º–µ–Ω–∞: MatOnto ===\n",
      "Train: 85 —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
      "–ü–æ–ª—É—á–µ–Ω–∏–µ train —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:25<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Train/Train –º–∞—Ç—Ä–∏—Ü—ã...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 44.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: /home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/src/taskB/method_v1_1/MatOnto_train_train_scores.csv\n",
      "Test: 37 —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
      "–ü–æ–ª—É—á–µ–Ω–∏–µ test —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:07<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Test/Train –º–∞—Ç—Ä–∏—Ü—ã...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 135.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: /home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/src/taskB/method_v1_1/MatOnto_test_train_scores.csv\n",
      "–ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–º–µ–Ω–∞: MatOnto\n",
      "\n",
      "=== –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–º–µ–Ω–∞: OBI ===\n",
      "Train: 201 —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
      "–ü–æ–ª—É—á–µ–Ω–∏–µ train —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [01:04<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Train/Train –º–∞—Ç—Ä–∏—Ü—ã...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 634.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: /home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/src/taskB/method_v1_1/OBI_train_train_scores.csv\n",
      "Test: 87 —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
      "–ü–æ–ª—É—á–µ–Ω–∏–µ test —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:20<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Test/Train –º–∞—Ç—Ä–∏—Ü—ã...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: /home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/src/taskB/method_v1_1/OBI_test_train_scores.csv\n",
      "–ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–º–µ–Ω–∞: OBI\n",
      "\n",
      "=== –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–º–µ–Ω–∞: SWEET ===\n",
      "Train: 1558 —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
      "–ü–æ–ª—É—á–µ–Ω–∏–µ train —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 195/195 [06:17<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Train/Train –º–∞—Ç—Ä–∏—Ü—ã...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 22.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: /home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/src/taskB/method_v1_1/SWEET_train_train_scores.csv\n",
      "Test: 626 —Ç–µ—Ä–º–∏–Ω–æ–≤\n",
      "–ü–æ–ª—É—á–µ–Ω–∏–µ test —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [02:16<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—ã—á–∏—Å–ª–µ–Ω–∏–µ Test/Train –º–∞—Ç—Ä–∏—Ü—ã...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarity: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 94.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: /home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/src/taskB/method_v1_1/SWEET_test_train_scores.csv\n",
      "–ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–º–µ–Ω–∞: SWEET\n"
     ]
    }
   ],
   "source": [
    "# 4. Domain Processing: Generate Embeddings and Similarity Matrices\n",
    "\n",
    "def process_domain_embeddings(domain: str, train_data: List[Dict], test_data: List[Dict] = None):\n",
    "    \"\"\"Processes a single domain: generates embeddings and similarity matrices\"\"\"\n",
    "    print(f\"\\n=== Processing domain: {domain} ===\")\n",
    "    \n",
    "    # Prepare texts for embedding\n",
    "    instruction = \"Given a term, find similar terms that have related semantic types or categories.\"\n",
    "    \n",
    "    # Train data\n",
    "    train_terms = [item['term'] for item in train_data]\n",
    "    train_texts_with_types = [f\"Term: {item['term']}, Types: {', '.join(item['types'])}\" for item in train_data]\n",
    "    train_texts_instruct = [get_detailed_instruct(instruction, text) for text in train_texts_with_types]\n",
    "    \n",
    "    print(f\"Train: {len(train_terms)} terms\")\n",
    "    \n",
    "    # Generate embeddings for train set\n",
    "    print(\"Generating train embeddings...\")\n",
    "    train_embeddings = get_embeddings_batch(train_texts_instruct, model, tokenizer, batch_size=8)\n",
    "    \n",
    "    # Train/Train similarity matrix\n",
    "    print(\"Computing Train/Train similarity matrix...\")\n",
    "    train_train_scores = compute_similarity_matrix(train_embeddings, train_embeddings, batch_size=100)\n",
    "    \n",
    "    # Save Train/Train matrix\n",
    "    train_train_df = pd.DataFrame(\n",
    "        train_train_scores.numpy(), \n",
    "        columns=train_terms, \n",
    "        index=train_terms\n",
    "    )\n",
    "    train_train_path = f'../../../src/taskB/method_v1_1/{domain}_train_train_scores.csv'\n",
    "    train_train_df.to_csv(train_train_path)\n",
    "    print(f\"Saved: {train_train_path}\")\n",
    "    \n",
    "    # Test data (if available)\n",
    "    if test_data:\n",
    "        test_terms = [item['term'] for item in test_data]\n",
    "        test_texts_with_types = [f\"Term: {item['term']}\" for item in test_data]  # Test data has no types\n",
    "        test_texts_instruct = [get_detailed_instruct(instruction, text) for text in test_texts_with_types]\n",
    "        \n",
    "        print(f\"Test: {len(test_terms)} terms\")\n",
    "        \n",
    "        # Generate embeddings for test set\n",
    "        print(\"Generating test embeddings...\")\n",
    "        test_embeddings = get_embeddings_batch(test_texts_instruct, model, tokenizer, batch_size=8)\n",
    "        \n",
    "        # Test/Train similarity matrix\n",
    "        print(\"Computing Test/Train similarity matrix...\")\n",
    "        test_train_scores = compute_similarity_matrix(test_embeddings, train_embeddings, batch_size=100)\n",
    "        \n",
    "        # Save Test/Train matrix\n",
    "        test_train_df = pd.DataFrame(\n",
    "            test_train_scores.numpy(), \n",
    "            columns=train_terms, \n",
    "            index=test_terms\n",
    "        )\n",
    "        test_train_path = f'../../../src/taskB/method_v1_1/{domain}_test_train_scores.csv'\n",
    "        test_train_df.to_csv(test_train_path)\n",
    "        print(f\"Saved: {test_train_path}\")\n",
    "    \n",
    "    # Free up memory\n",
    "    del train_embeddings, train_train_scores\n",
    "    if test_data:\n",
    "        del test_embeddings, test_train_scores\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Finished processing domain: {domain}\")\n",
    "\n",
    "# Process all domains\n",
    "for domain in ['MatOnto', 'OBI', 'SWEET']:\n",
    "    process_domain_embeddings(domain, train_data[domain], test_data[domain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –ó–∞–ø—É—Å–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è TaskB –±–µ–∑ —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫...\n",
      "\n",
      "=== –û–±—Ä–∞–±–æ—Ç–∫–∞ MatOnto ===\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ 85 train –ø—Ä–∏–º–µ—Ä–æ–≤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MatOnto train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85/85 [00:00<00:00, 1831.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã train –¥–∞–Ω–Ω—ã–µ: /home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/2025/TaskB-TermTyping/MatOnto/train/term_typing_train_data.json\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ 37 test –ø—Ä–∏–º–µ—Ä–æ–≤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MatOnto test:   0%|          | 0/37 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MatOnto test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [00:00<00:00, 6384.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã test –¥–∞–Ω–Ω—ã–µ: /home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/2025/TaskB-TermTyping/MatOnto/test/terms2types.json\n",
      "–ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ MatOnto\n",
      "\n",
      "=== –û–±—Ä–∞–±–æ—Ç–∫–∞ OBI ===\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ 201 train –ø—Ä–∏–º–µ—Ä–æ–≤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OBI train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 201/201 [00:00<00:00, 3047.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã train –¥–∞–Ω–Ω—ã–µ: /home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/2025/TaskB-TermTyping/OBI/train/term_typing_train_data.json\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ 87 test –ø—Ä–∏–º–µ—Ä–æ–≤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing OBI test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87/87 [00:00<00:00, 6443.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã test –¥–∞–Ω–Ω—ã–µ: /home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/2025/TaskB-TermTyping/OBI/test/terms2types.json\n",
      "–ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ OBI\n",
      "\n",
      "=== –û–±—Ä–∞–±–æ—Ç–∫–∞ SWEET ===\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ 1558 train –ø—Ä–∏–º–µ—Ä–æ–≤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SWEET train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1558/1558 [00:00<00:00, 2001.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã train –¥–∞–Ω–Ω—ã–µ: /home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/2025/TaskB-TermTyping/SWEET/train/term_typing_train_data.json\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ 626 test –ø—Ä–∏–º–µ—Ä–æ–≤...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing SWEET test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 626/626 [00:00<00:00, 4130.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã test –¥–∞–Ω–Ω—ã–µ: /home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/2025/TaskB-TermTyping/SWEET/test/terms2types.json\n",
      "–ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ SWEET\n",
      "\n",
      "üéâ –°–æ–∑–¥–∞–Ω–∏–µ RAG –¥–∞–Ω–Ω—ã—Ö –¥–ª—è TaskB –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "def create_taskb_terms2types_with_rag_fixed():\n",
    "    \"\"\"Creates terms2types.json files with RAG data for TaskB (corrected version without cyclic references)\"\"\"\n",
    "    \n",
    "    # Valid domains for TaskB\n",
    "    domains = ['MatOnto', 'OBI', 'SWEET']\n",
    "    \n",
    "    # Process each domain\n",
    "    for domain in domains:\n",
    "        print(f\"\\n=== Processing {domain} ===\")\n",
    "        \n",
    "        # Load TaskB train data\n",
    "        train_path = f'/home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/2025/TaskB-TermTyping/{domain}/train/term_typing_train_data.json'\n",
    "        if not os.path.exists(train_path):\n",
    "            print(f\"Train data not found: {train_path}\")\n",
    "            continue\n",
    "            \n",
    "        with open(train_path, 'r', encoding='utf-8') as f:\n",
    "            train_data_domain = json.load(f)\n",
    "        \n",
    "        # Create a lookup dictionary (WITHOUT RAG field to avoid cyclic references)\n",
    "        train_dict = {}\n",
    "        for item in train_data_domain:\n",
    "            # Create a clean copy without RAG field\n",
    "            clean_item = {k: v for k, v in item.items() if k != 'RAG'}\n",
    "            train_dict[item['term']] = clean_item\n",
    "        \n",
    "        # Load similarity matrix (correct path for TaskB)\n",
    "        scores_path = f'/home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/src/taskB/method_v1_1/{domain}_train_train_scores.csv'\n",
    "        if not os.path.exists(scores_path):\n",
    "            print(f\"Similarity matrix not found: {scores_path}\")\n",
    "            continue\n",
    "            \n",
    "        scores_df = pd.read_csv(scores_path, index_col=0)\n",
    "        \n",
    "        # Simplified function to find similar terms\n",
    "        def find_top_similar_terms(scores_df, term, top_k=10, exclude_self=True):\n",
    "            if term not in scores_df.index:\n",
    "                return []\n",
    "            try:\n",
    "                # Simple approach: use .loc and process the result\n",
    "                scores_row = scores_df.loc[term]\n",
    "                \n",
    "                # If duplicated indices yield a DataFrame, take the first row\n",
    "                if isinstance(scores_row, pd.DataFrame):\n",
    "                    scores_row = scores_row.iloc[0]\n",
    "                \n",
    "                # Remove the term itself if needed\n",
    "                if exclude_self and term in scores_row.index:\n",
    "                    scores_row = scores_row.drop(term)\n",
    "                \n",
    "                # Sort descending and get top-k\n",
    "                top_terms = scores_row.sort_values(ascending=False).head(top_k)\n",
    "                return top_terms.index.tolist()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing term '{term}': {e}\")\n",
    "                return []\n",
    "        \n",
    "        # Process train data\n",
    "        print(f\"Processing {len(train_data_domain)} train examples...\")\n",
    "        for item in tqdm(train_data_domain, desc=f\"Processing {domain} train\"):\n",
    "            term = item['term']\n",
    "            similar_terms = find_top_similar_terms(scores_df, term, top_k=10, exclude_self=True)\n",
    "            \n",
    "            # Add RAG examples (clean copies)\n",
    "            rag_examples = []\n",
    "            for similar_term in similar_terms:\n",
    "                if similar_term in train_dict:\n",
    "                    # Use deep copy to fully avoid shared references\n",
    "                    rag_examples.append(copy.deepcopy(train_dict[similar_term]))\n",
    "            item['RAG'] = rag_examples\n",
    "        \n",
    "        # Save updated train data\n",
    "        with open(train_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(train_data_domain, f, ensure_ascii=False, indent=1)\n",
    "        print(f\"‚úÖ Train data saved: {train_path}\")\n",
    "        \n",
    "        # Process test data\n",
    "        domain_lower = domain.lower()\n",
    "        test_path = f'/home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/2025/TaskB-TermTyping/{domain}/test/{domain_lower}_term_typing_test_data.json'\n",
    "        test_scores_path = f'/home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/src/taskB/method_v1_1/{domain}_test_train_scores.csv'\n",
    "        \n",
    "        if os.path.exists(test_path) and os.path.exists(test_scores_path):\n",
    "            with open(test_path, 'r', encoding='utf-8') as f:\n",
    "                test_json_data = json.load(f)\n",
    "            \n",
    "            test_data_domain = [{\"term\": item[\"term\"], \"types\": []} for item in test_json_data]\n",
    "            test_scores_df = pd.read_csv(test_scores_path, index_col=0)\n",
    "            \n",
    "            print(f\"Processing {len(test_data_domain)} test examples...\")\n",
    "            for item in tqdm(test_data_domain, desc=f\"Processing {domain} test\"):\n",
    "                term = item['term']\n",
    "                if term in test_scores_df.index:\n",
    "                    try:\n",
    "                        scores_row = test_scores_df.loc[term]\n",
    "                        if isinstance(scores_row, pd.DataFrame):\n",
    "                            scores_row = scores_row.iloc[0]\n",
    "                        \n",
    "                        top_train_terms = scores_row.sort_values(ascending=False).head(10).index.tolist()\n",
    "                        rag_examples = []\n",
    "                        for train_term in top_train_terms:\n",
    "                            if train_term in train_dict:\n",
    "                                rag_examples.append(copy.deepcopy(train_dict[train_term]))\n",
    "                        item['RAG'] = rag_examples\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing test term '{term}': {e}\")\n",
    "                        item['RAG'] = []\n",
    "                else:\n",
    "                    item['RAG'] = []\n",
    "            \n",
    "            # Save test data with RAG\n",
    "            test_output_path = f'/home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/2025/TaskB-TermTyping/{domain}/test/terms2types.json'\n",
    "            os.makedirs(os.path.dirname(test_output_path), exist_ok=True)\n",
    "            with open(test_output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(test_data_domain, f, ensure_ascii=False, indent=1)\n",
    "            print(f\"‚úÖ Test data saved: {test_output_path}\")\n",
    "        else:\n",
    "            print(\"Test data or scores not found:\")\n",
    "            print(f\"  Test: {test_path} - {'‚úÖ' if os.path.exists(test_path) else '‚ùå'}\")\n",
    "            print(f\"  Scores: {test_scores_path} - {'‚úÖ' if os.path.exists(test_scores_path) else '‚ùå'}\")\n",
    "        \n",
    "        print(f\"Finished processing {domain}\")\n",
    "    \n",
    "    print(\"\\nüéâ RAG data creation for TaskB completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üîÑ Running the corrected TaskB function without cyclic references...\")\n",
    "    create_taskb_terms2types_with_rag_fixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –ó–∞–ø—É—Å–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è test –¥–∞–Ω–Ω—ã—Ö...\n",
      "\n",
      "=== –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ MatOnto ===\n",
      "–ò—Å—Ö–æ–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: 37\n",
      "–¢–µ–∫—É—â–∏—Ö –∑–∞–ø–∏—Å–µ–π: 37\n",
      "‚ö†Ô∏è  MatOnto - —Ç–µ—Ä–º–∏–Ω 'newton' (id: TT_5a5763f5): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  MatOnto - —Ç–µ—Ä–º–∏–Ω 'newton per meter' (id: TT_ce5417fc): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  MatOnto - —Ç–µ—Ä–º–∏–Ω 'second power 4' (id: TT_ce2b537c): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  MatOnto - —Ç–µ—Ä–º–∏–Ω 'radian' (id: TT_f8e74193): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  MatOnto - —Ç–µ—Ä–º–∏–Ω 'becquerel' (id: TT_a0402f84): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  MatOnto - —Ç–µ—Ä–º–∏–Ω 'unit_kelvin_-1' (id: TT_4c87c381): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  MatOnto - —Ç–µ—Ä–º–∏–Ω 'second per mole' (id: TT_ab85a470): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  MatOnto - —Ç–µ—Ä–º–∏–Ω 'hertz' (id: TT_5ba78869): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  MatOnto - —Ç–µ—Ä–º–∏–Ω 'unit_gigapascal' (id: TT_e707b15a): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  MatOnto - —Ç–µ—Ä–º–∏–Ω 'unit_megapascal' (id: TT_fac88931): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  MatOnto - —Ç–µ—Ä–º–∏–Ω 'reciprocal second' (id: TT_300a8cf0): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  MatOnto - —Ç–µ—Ä–º–∏–Ω 'gray per second' (id: TT_4e46b6d7): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ 37 –∑–∞–ø–∏—Å–µ–π\n",
      "üìä RAG —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n",
      "   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: 37\n",
      "   –ú–∏–Ω RAG: 9\n",
      "   –ú–∞–∫—Å RAG: 10\n",
      "   –°—Ä–µ–¥–Ω–µ–µ RAG: 9.68\n",
      "   –ó–∞–ø–∏—Å–µ–π —Å 10 RAG: 25\n",
      "   –ó–∞–ø–∏—Å–µ–π —Å != 10 RAG: 12\n",
      "\n",
      "=== –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ OBI ===\n",
      "–ò—Å—Ö–æ–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: 87\n",
      "–¢–µ–∫—É—â–∏—Ö –∑–∞–ø–∏—Å–µ–π: 87\n",
      "‚ö†Ô∏è  OBI - —Ç–µ—Ä–º–∏–Ω 'eBioscience' (id: TT_d539e3e1): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  OBI - —Ç–µ—Ä–º–∏–Ω 'Antigenix' (id: TT_05e33e44): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  OBI - —Ç–µ—Ä–º–∏–Ω 'BioGents' (id: TT_47e2f629): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  OBI - —Ç–µ—Ä–º–∏–Ω 'Advanced Instruments Inc. (AI Companies)' (id: TT_90fdf1cb): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  OBI - —Ç–µ—Ä–º–∏–Ω 'Luminex' (id: TT_2afe465c): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  OBI - —Ç–µ—Ä–º–∏–Ω 'Illumina' (id: TT_f404121d): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  OBI - —Ç–µ—Ä–º–∏–Ω 'ACS' (id: TT_2cb446e6): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  OBI - —Ç–µ—Ä–º–∏–Ω 'AES Chemunex' (id: TT_d6dcaedc): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ 87 –∑–∞–ø–∏—Å–µ–π\n",
      "üìä RAG —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n",
      "   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: 87\n",
      "   –ú–∏–Ω RAG: 9\n",
      "   –ú–∞–∫—Å RAG: 10\n",
      "   –°—Ä–µ–¥–Ω–µ–µ RAG: 9.91\n",
      "   –ó–∞–ø–∏—Å–µ–π —Å 10 RAG: 79\n",
      "   –ó–∞–ø–∏—Å–µ–π —Å != 10 RAG: 8\n",
      "\n",
      "=== –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ SWEET ===\n",
      "–ò—Å—Ö–æ–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: 626\n",
      "–¢–µ–∫—É—â–∏—Ö –∑–∞–ø–∏—Å–µ–π: 626\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'sigma' (id: TT_01c7707e): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'category2' (id: TT_e2be01a5): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'acidify' (id: TT_fca5f9f9): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'beryllium' (id: TT_152f3a8d): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'binder' (id: TT_a0dc5361): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'uva' (id: TT_e8d6b32f): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'dendritic' (id: TT_b5b09234): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'dim' (id: TT_7ce655e9): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'acceptable' (id: TT_7ff90784): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'pitch' (id: TT_6135b3e1): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'ascii' (id: TT_9de88d2d): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'c10h16' (id: TT_ad1bbe57): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'hdfeos2' (id: TT_5c033064): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'he1083' (id: TT_1091e607): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'hco3minus' (id: TT_d1fb6213): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'negative' (id: TT_9c8ac160): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'remote' (id: TT_22e1ddd0): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'even' (id: TT_e179b811): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'he4' (id: TT_583c6331): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'von karman constant' (id: TT_7f00cf84): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'precise' (id: TT_2c72a2bd): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'semi precious' (id: TT_b8e624c9): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'light' (id: TT_ec64041e): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'pan' (id: TT_49de527d): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'clockwise' (id: TT_4b97f1ec): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'ef0' (id: TT_a4d6b9e2): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'priabonian' (id: TT_eda17fd6): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'ch4' (id: TT_d1e324ef): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'anthropogenic' (id: TT_a4fc1bf8): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'tiny' (id: TT_238f77a8): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'cadmium' (id: TT_6de53a88): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'category4' (id: TT_3e9eaf0e): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'dwb' (id: TT_8bb56661): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'thermal plasma' (id: TT_498b53c9): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'basin groups' (id: TT_efe70492): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'bwk' (id: TT_e147d398): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'dynamic' (id: TT_4e3c03ca): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'reynolds number' (id: TT_ac3340ac): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'analyzed' (id: TT_29a10f10): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'unacceptable' (id: TT_a7579eb1): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'inert' (id: TT_fd1d6b94): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'hazard' (id: TT_13835c44): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'hbeta' (id: TT_c3122e54): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'ef5' (id: TT_2d6bbd7d): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'sidereal' (id: TT_81671ca6): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'connected' (id: TT_449c56bb): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'non thermal plasma' (id: TT_e57fb4fb): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'cak' (id: TT_2bbcc3cb): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'hydrobromous acid' (id: TT_0a916ef2): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'vertical' (id: TT_1a5d1ce3): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'c2h6o' (id: TT_9a8c04ca): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'indicator' (id: TT_f4a40e4d): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'selandian' (id: TT_1dbfa101): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'enrich' (id: TT_0ea60382): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'halosteric' (id: TT_28165e62): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'bashkirian' (id: TT_61e6d5a1): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'disaster' (id: TT_90a49eb6): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'pentad' (id: TT_5c920fb8): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'rhuddanian' (id: TT_386ec924): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'drumian' (id: TT_4f1c6fe7): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'no3minus' (id: TT_83ca6886): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'piacenzian' (id: TT_e432bce3): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'copper' (id: TT_63262db4): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'brno3' (id: TT_d9536b24): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'prior' (id: TT_419a37a9): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'equator' (id: TT_c6262816): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'hybrid sigma' (id: TT_4a2916b8): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'nfma' (id: TT_094dc640): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'semi' (id: TT_caf65747): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'x4 class' (id: TT_433a6162): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'hno3' (id: TT_a57744b1): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'total' (id: TT_a5a76e1f): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'equatorial' (id: TT_2ad655c1): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'sun synchronous' (id: TT_796a74a4): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'very' (id: TT_5b582857): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'he304' (id: TT_1c18e3bb): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'broad' (id: TT_e2869a95): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'ethene' (id: TT_c6e62afd): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'nitric oxide' (id: TT_dccbed46): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'zone code' (id: TT_752b69f1): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'synoptic' (id: TT_cc9fca33): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'triaxial' (id: TT_c4f21e14): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'dapingian' (id: TT_616a783b): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'bsh' (id: TT_594e4c76): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'longitude' (id: TT_10ffc87b): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'psd' (id: TT_17afa117): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'arff' (id: TT_46502cd6): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'biaxial' (id: TT_ac0ff00a): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'c12' (id: TT_b9cf4a5b): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'emerging' (id: TT_0d16d6ef): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'nfs' (id: TT_a31e7083): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'cisuralian' (id: TT_9cdcaa9d): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'visible' (id: TT_13fc695d): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'violet' (id: TT_6c7c19aa): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'indirect' (id: TT_0d0e9987): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'disasterous' (id: TT_7404313a): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'trigonal' (id: TT_e984cede): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'void' (id: TT_6cb0388a): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'gelasian' (id: TT_87174f1a): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'lte' (id: TT_66f8101d): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'chattian' (id: TT_d937cdce): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'u238' (id: TT_dffacd35): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'severe' (id: TT_4ea59887): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'serravallian' (id: TT_d5cdb49f): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'neon' (id: TT_4eda8ff9): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'sonic mach number' (id: TT_9fcdfe3e): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'o18' (id: TT_a167723a): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'acrylonitrile' (id: TT_52f53eb6): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'acrolein' (id: TT_02158935): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'deuterium' (id: TT_eaf26b59): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'compact' (id: TT_4363869f): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'specific retention' (id: TT_04bcd131): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'latitude' (id: TT_d5288fcb): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'h10' (id: TT_f16225dc): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'published' (id: TT_54e29617): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'c2h6' (id: TT_4610dbef): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'tropics' (id: TT_175c85af): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'micron' (id: TT_f7557642): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'ethane' (id: TT_c28bffeb): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'dwd' (id: TT_db03f52d): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'burn' (id: TT_78770291): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'sheinwoodian' (id: TT_bb3cad11): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'aluminum' (id: TT_77a1a730): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 's32' (id: TT_96912c6c): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'non methane hydrocarbon' (id: TT_8a8d0873): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'host' (id: TT_e6a5222b): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'safe' (id: TT_9d88fc3c): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'slightly' (id: TT_8f99edaa): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'anoxic' (id: TT_7f70976c): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'tac' (id: TT_b0ca24a8): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'fe2.3' (id: TT_d4cab41a): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'dwb' (id: TT_8bb56661): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'independent' (id: TT_4e4eef19): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'refractive index' (id: TT_66498bf9): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'th231' (id: TT_ae276477): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'leaf area index' (id: TT_9a696758): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'no3' (id: TT_5d38f17b): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'nox' (id: TT_a6279275): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'dfc' (id: TT_55705978): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'ku band' (id: TT_9e99064e): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'pb206' (id: TT_9a8b99a3): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'tertiary' (id: TT_5e2d045f): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'microwave' (id: TT_fe68f28c): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'c2h6s' (id: TT_d77621e8): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'c14' (id: TT_6e4530c0): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'noy' (id: TT_41ca3c25): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 's36' (id: TT_46a5c696): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'c6h6' (id: TT_fa75de12): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'al26' (id: TT_478b6356): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'sensible' (id: TT_5d844340): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'pb202' (id: TT_7f5f4a19): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'transit' (id: TT_55f474ac): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'x45 class' (id: TT_366ae90f): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'v band' (id: TT_feec0db3): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'passive' (id: TT_081502f5): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'visean' (id: TT_4db26c71): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'kungurian' (id: TT_83d744bc): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'he3' (id: TT_6b380c65): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'opposition' (id: TT_226a0085): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'radioactive decay' (id: TT_fda19db9): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'irrigated' (id: TT_e457ad55): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'methane' (id: TT_599f422e): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'co3minusminus' (id: TT_bfd638aa): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'gzhelian' (id: TT_1e29ff2b): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'tropical savanna climate' (id: TT_a28f8d1b): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'sakmarian' (id: TT_132cec32): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'berriasian' (id: TT_354827c9): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'nad2' (id: TT_3eb51fd2): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'violent' (id: TT_53fa014e): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'south pole' (id: TT_37299a2d): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'hclo4' (id: TT_9ef9897d): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'x49 class' (id: TT_78dca7c3): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'th234' (id: TT_f809eb1b): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'n14' (id: TT_e6f7c502): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'ka band' (id: TT_ed9c0294): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'hclo4' (id: TT_9ef9897d): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'dms' (id: TT_93b9b2bb): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'tropical rain forest climate' (id: TT_b799d051): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'nano' (id: TT_8c4570cc): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'methanoic acid' (id: TT_518ce6c4): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'stenian' (id: TT_18353a89): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'retardation factor' (id: TT_e8bdbb83): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'u238' (id: TT_dffacd35): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'secular' (id: TT_be543083): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'fnu' (id: TT_b7d86443): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'electronic' (id: TT_c04f30ec): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'zenith' (id: TT_f6dc9c6e): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'k41' (id: TT_293f559e): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'dead' (id: TT_0ed0295a): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'x47 class' (id: TT_1bc143e7): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'robinson' (id: TT_f75b0090): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'north pole' (id: TT_0c882d10): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'metallic' (id: TT_8079a887): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'c3h8' (id: TT_777105aa): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'chromium' (id: TT_f70ab457): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'gentle' (id: TT_95cdaebc): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'stressor' (id: TT_8d61c5c2): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 's34' (id: TT_e1af4347): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'central dense overcast' (id: TT_125ec8a6): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'x42 class' (id: TT_42fb6810): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'cos' (id: TT_e9354ba4): 7 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'north' (id: TT_79f3f382): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'geostationary' (id: TT_951722ac): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'rouse number' (id: TT_3a1937c1): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'trace' (id: TT_9ab9afbc): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'nitrogen monoxide' (id: TT_ec2a8602): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'uvb' (id: TT_283e33c2): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'pb204' (id: TT_3747460e): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'orange' (id: TT_53426ba9): 8 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚ö†Ô∏è  SWEET - —Ç–µ—Ä–º–∏–Ω 'silicon dioxide' (id: TT_1b4c6276): 9 RAG –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ 10\n",
      "‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ 626 –∑–∞–ø–∏—Å–µ–π\n",
      "üìä RAG —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n",
      "   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: 626\n",
      "   –ú–∏–Ω RAG: 7\n",
      "   –ú–∞–∫—Å RAG: 10\n",
      "   –°—Ä–µ–¥–Ω–µ–µ RAG: 9.48\n",
      "   –ó–∞–ø–∏—Å–µ–π —Å 10 RAG: 416\n",
      "   –ó–∞–ø–∏—Å–µ–π —Å != 10 RAG: 210\n",
      "\n",
      "üéâ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "\n",
      "üìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ RAG –ø—Ä–∏–º–µ—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é 'rag_statistics'\n",
      "–î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–æ–º–µ–Ω—ã: ['MatOnto', 'OBI', 'SWEET']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def fix_test_data_add_id_and_check_rag():\n",
    "    \"\"\"Fixes test data: adds 'id' field and checks the number of RAG examples\"\"\"\n",
    "    \n",
    "    domains = ['MatOnto', 'OBI', 'SWEET']\n",
    "    rag_stats = defaultdict(list)  # Statistics on number of RAG examples\n",
    "    \n",
    "    for domain in domains:\n",
    "        print(f\"\\n=== Fixing {domain} ===\")\n",
    "        \n",
    "        # File paths\n",
    "        domain_lower = domain.lower()\n",
    "        original_test_path = f'/home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/2025/TaskB-TermTyping/{domain}/test/{domain_lower}_term_typing_test_data.json'\n",
    "        current_test_path = f'/home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/2025/TaskB-TermTyping/{domain}/test/terms2types.json'\n",
    "        \n",
    "        # Check file existence\n",
    "        if not os.path.exists(original_test_path):\n",
    "            print(f\"‚ùå Original test file not found: {original_test_path}\")\n",
    "            continue\n",
    "            \n",
    "        if not os.path.exists(current_test_path):\n",
    "            print(f\"‚ùå Current test file not found: {current_test_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Load original data (with id)\n",
    "        with open(original_test_path, 'r', encoding='utf-8') as f:\n",
    "            original_data = json.load(f)\n",
    "        \n",
    "        # Load current data (with RAG but without id)\n",
    "        with open(current_test_path, 'r', encoding='utf-8') as f:\n",
    "            current_data = json.load(f)\n",
    "        \n",
    "        print(f\"Original entries: {len(original_data)}\")\n",
    "        print(f\"Current entries: {len(current_data)}\")\n",
    "        \n",
    "        # Build a quick lookup dictionary by term\n",
    "        original_dict = {item['term']: item for item in original_data}\n",
    "        \n",
    "        # Fix data: add id and check RAG\n",
    "        fixed_data = []\n",
    "        missing_ids = []\n",
    "        \n",
    "        for i, current_item in enumerate(current_data):\n",
    "            term = current_item['term']\n",
    "            \n",
    "            # Look up the corresponding original item\n",
    "            if term in original_dict:\n",
    "                original_item = original_dict[term]\n",
    "                \n",
    "                # Construct the corrected item\n",
    "                fixed_item = {\n",
    "                    'id': original_item['id'],  # Add id from original data\n",
    "                    'term': current_item['term'],\n",
    "                    'types': current_item.get('types', []),\n",
    "                    'RAG': current_item.get('RAG', [])\n",
    "                }\n",
    "                \n",
    "                # Check number of RAG examples\n",
    "                rag_count = len(fixed_item['RAG'])\n",
    "                rag_stats[domain].append(rag_count)\n",
    "                \n",
    "                if rag_count != 10:\n",
    "                    print(f\"‚ö†Ô∏è  {domain} - term '{term}' (id: {fixed_item['id']}): {rag_count} RAG examples instead of 10\")\n",
    "                \n",
    "                fixed_data.append(fixed_item)\n",
    "            else:\n",
    "                print(f\"‚ùå Term '{term}' not found in original data!\")\n",
    "                missing_ids.append(term)\n",
    "                \n",
    "                # Add fallback with missing id\n",
    "                fixed_item = {\n",
    "                    'id': f\"MISSING_ID_{i}\",\n",
    "                    'term': current_item['term'],\n",
    "                    'types': current_item.get('types', []),\n",
    "                    'RAG': current_item.get('RAG', [])\n",
    "                }\n",
    "                fixed_data.append(fixed_item)\n",
    "        \n",
    "        # Save fixed data\n",
    "        with open(current_test_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(fixed_data, f, ensure_ascii=False, indent=1)\n",
    "        \n",
    "        print(f\"‚úÖ Fixed {len(fixed_data)} records\")\n",
    "        if missing_ids:\n",
    "            print(f\"‚ùå Missing id for {len(missing_ids)} terms: {missing_ids}\")\n",
    "        \n",
    "        # RAG statistics\n",
    "        if rag_stats[domain]:\n",
    "            rag_counts = rag_stats[domain]\n",
    "            print(f\"üìä RAG statistics:\")\n",
    "            print(f\"   Total records: {len(rag_counts)}\")\n",
    "            print(f\"   Min RAG: {min(rag_counts)}\")\n",
    "            print(f\"   Max RAG: {max(rag_counts)}\")\n",
    "            print(f\"   Avg RAG: {sum(rag_counts)/len(rag_counts):.2f}\")\n",
    "            print(f\"   Records with 10 RAG: {rag_counts.count(10)}\")\n",
    "            print(f\"   Records with != 10 RAG: {len(rag_counts) - rag_counts.count(10)}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Fixing completed!\")\n",
    "    \n",
    "    # Return RAG statistics for further analysis\n",
    "    return dict(rag_stats)\n",
    "\n",
    "# Run the fix\n",
    "print(\"üîÑ Running test data fix...\")\n",
    "rag_statistics = fix_test_data_add_id_and_check_rag()\n",
    "\n",
    "# Save statistics for later use\n",
    "print(f\"\\nüìà RAG statistics saved to variable 'rag_statistics'\")\n",
    "print(f\"Available domains: {list(rag_statistics.keys())}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rah_python312_cuda124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
