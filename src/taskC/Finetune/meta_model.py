"""
Qwen3CrossAttentionMetaModel - –º–µ—Ç–∞-–º–æ–¥–µ–ª—å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è Qwen3 –∏ CrossAttentionModel

–≠—Ç–∞ –º–æ–¥–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑, –ø–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ Qwen3,
–∞ –∑–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç CrossAttentionModel –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –≤–Ω–∏–º–∞–Ω–∏—è.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor
from transformers import AutoTokenizer, AutoModel
from typing import List, Dict, Tuple, Optional, Any, Union
import json
import os
from pathlib import Path
import numpy as np
import logging

from cross_attention_model import CrossAttentionModel


def last_token_pool(
    last_hidden_states: Tensor,
     attention_mask: Tensor) -> Tensor:
    """Extract last token embeddings with proper handling of padding"""
    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
    if left_padding:
        return last_hidden_states[:, -1]
    else:
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_states.shape[0]
        return last_hidden_states[torch.arange(
            batch_size, device=last_hidden_states.device), sequence_lengths]


class Qwen3CrossAttentionMetaModel(nn.Module):
    """
    –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∞—è Qwen3 –∏ CrossAttentionModel

    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –¥–≤–∞ —Å–ø–∏—Å–∫–∞ —Ñ—Ä–∞–∑ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –≤–Ω–∏–º–∞–Ω–∏—è –º–µ–∂–¥—É –Ω–∏–º–∏.

    Args:
        cross_attention_model: –æ–±—É—á–µ–Ω–Ω–∞—è CrossAttentionModel
        qwen3_model_name: –∏–º—è –º–æ–¥–µ–ª–∏ Qwen3 –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        max_length: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
        embedding_batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        freeze_qwen3: –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å –ª–∏ –≤–µ—Å–∞ Qwen3
        device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        embedding_pooling: –º–µ—Ç–æ–¥ pooling –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ("mean", "last_token", "last_token_norm")
    """

    def __init__(
        self,
        qwen3_model_name: str = "Qwen/Qwen3-Embedding-0.6B",
        max_length: int = 512,
        embedding_batch_size: int = 32,
        freeze_qwen3: bool = True,
        device: str = "auto",
        embedding_pooling: str = "last_token_norm",
        cross_attention_model: Union[CrossAttentionModel, None, str] = None,
    ):
        super().__init__()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è pooling –º–µ—Ç–æ–¥–∞
        if embedding_pooling not in ["mean", "last_token", "last_token_norm"]:
            raise ValueError(
                f"embedding_pooling –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'mean' –∏–ª–∏ 'last_token', –ø–æ–ª—É—á–µ–Ω: {embedding_pooling}")

        self.qwen3_model_name = qwen3_model_name
        self.max_length = max_length
        self.embedding_batch_size = embedding_batch_size
        self.freeze_qwen3 = freeze_qwen3
        self.embedding_pooling = embedding_pooling

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º f1_threshold –∫–∞–∫ None (–±—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
        # –∏–∑ checkpoint)
        self.f1_threshold = None

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        if device == "auto":
            self.device = torch.device(
    "cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)

        # –ó–∞–≥—Ä—É–∑–∫–∞ Qwen3 –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        self.tokenizer = AutoTokenizer.from_pretrained(qwen3_model_name)
        self.qwen3_model = AutoModel.from_pretrained(qwen3_model_name)

        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –≤–µ—Å–∞ Qwen3 –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if freeze_qwen3:
            for param in self.qwen3_model.parameters():
                param.requires_grad = False

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º CrossAttentionModel
        if cross_attention_model is None:
            cross_attention_model = CrossAttentionModel(
                self.qwen3_model.config)
            self.cross_attention_model = cross_attention_model
        elif isinstance(cross_attention_model, str):
            self.cross_attention_model = CrossAttentionModel.from_pretrained(
                cross_attention_model)
        elif isinstance(cross_attention_model, CrossAttentionModel):
            self.cross_attention_model = cross_attention_model
        else:
            raise ValueError(
                f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø cross_attention_model: {type(cross_attention_model)}")

        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.qwen3_model = self.qwen3_model.to(self.device)
        self.cross_attention_model = self.cross_attention_model.to(self.device)

        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏
        self.qwen3_model.eval()

        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding_dim = self.qwen3_model.config.hidden_size

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
        if self.embedding_dim != cross_attention_model.hidden_size:
            raise ValueError(
    f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ Qwen3 ({self.embedding_dim}) "
    f"–Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é CrossAttentionModel ({cross_attention_model.hidden_size})")

    def get_embeddings(self, phrases: List[str]) -> torch.Tensor:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ñ—Ä–∞–∑

        Args:
            phrases: —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑

        Returns:
            embeddings: —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ (len(phrases), embedding_dim)
        """
        all_embeddings = []

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ—Ä–∞–∑—ã –±–∞—Ç—á–∞–º–∏
        for i in range(0, len(phrases), self.embedding_batch_size):
            batch_phrases = phrases[i:i + self.embedding_batch_size]

            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            with torch.no_grad():
                inputs = self.tokenizer(
                    batch_phrases,
                    padding=True,
                    truncation=True,
                    max_length=self.max_length,
                    return_tensors="pt"
                ).to(self.device)

                # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                outputs = self.qwen3_model(**inputs)

                # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ pooling
                if self.embedding_pooling == "mean":
                    # Mean pooling –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è
                    batch_embeddings = outputs.last_hidden_state.mean(dim=1)
                elif self.embedding_pooling == "last_token":
                    # Last token pooling —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π padding
                    batch_embeddings = last_token_pool(
                        outputs.last_hidden_state, inputs["attention_mask"])
                elif self.embedding_pooling == "last_token_norm":
                    # Last token pooling —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π padding
                    batch_embeddings = last_token_pool(
                        outputs.last_hidden_state, inputs["attention_mask"])
                    batch_embeddings = F.normalize(
                        batch_embeddings, p=2, dim=1)
                else:
                    raise ValueError(
                        f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ pooling: {self.embedding_pooling}")

                all_embeddings.append(batch_embeddings)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –±–∞—Ç—á–∏
        embeddings = torch.cat(all_embeddings, dim=0)

        return embeddings

    def forward(
        self,
        phrases_1: List[str],
        phrases_2: List[str]) -> torch.Tensor:
        """
        Forward pass –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏

        Args:
            phrases_1: –ø–µ—Ä–≤—ã–π —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑ (query)
            phrases_2: –≤—Ç–æ—Ä–æ–π —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑ (key)

        Returns:
            attention_matrix: –º–∞—Ç—Ä–∏—Ü–∞ –≤–Ω–∏–º–∞–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–º (len(phrases_1), len(phrases_2))
        """
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –æ–±–æ–∏—Ö —Å–ø–∏—Å–∫–æ–≤
        embeddings_1 = self.get_embeddings(phrases_1)
        embeddings_2 = self.get_embeddings(phrases_2)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º CrossAttentionModel
        attention_matrix = self.cross_attention_model(
            embeddings_1, embeddings_2)

        return attention_matrix

    def predict_relationships(
        self,
        phrases_1: List[str],
        phrases_2: List[str],
        threshold: Union[float, None] = None,
        remove_self_loops: bool = True
    ) -> List[Dict[str, str]]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –º–µ–∂–¥—É —Ñ—Ä–∞–∑–∞–º–∏ —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –ø–æ—Ä–æ–≥–∞

        Args:
            phrases_1: –ø–µ—Ä–≤—ã–π —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑ (–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –¥–µ—Ç–∏)
            phrases_2: –≤—Ç–æ—Ä–æ–π —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑ (–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–æ–¥–∏—Ç–µ–ª–∏)
            threshold: –ø–æ—Ä–æ–≥ –¥–ª—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π F1 threshold –∏–∑ –æ–±—É—á–µ–Ω–∏—è)
            remove_self_loops: —É–¥–∞–ª–∏—Ç—å –ª–∏ —Å–∞–º–æ–∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è (–∫–æ–≥–¥–∞ —Ñ—Ä–∞–∑–∞ —Å—Å—ã–ª–∞–µ—Ç—Å—è —Å–∞–º–∞ –Ω–∞ —Å–µ–±—è)

        Returns:
            relationships: —Å–ø–∏—Å–æ–∫ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"child": "...", "parent": "..."}]
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π F1 threshold –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω —è–≤–Ω–æ
        if threshold is None:
            threshold = getattr(self, 'f1_threshold', None)
            if threshold is None:
                threshold = 0.5

        # –ü–æ–ª—É—á–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –≤–Ω–∏–º–∞–Ω–∏—è
        attention_matrix = self.forward(phrases_1, phrases_2)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥
        binary_matrix = (attention_matrix > threshold).cpu().numpy()

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –Ω–µ–Ω—É–ª–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ)
        nonzero_indices = np.nonzero(binary_matrix)
        i_indices, j_indices = nonzero_indices[0], nonzero_indices[1]

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–Ω–æ—à–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–Ω—É–ª–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        relationships = []
        for idx in range(len(i_indices)):
            i, j = i_indices[idx], j_indices[idx]

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∞–º–æ–∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if remove_self_loops and phrases_1[i] == phrases_2[j]:
                continue

            relationships.append({
                "child": phrases_1[i],
                "parent": phrases_2[j],
                "confidence": float(attention_matrix[i, j].item())
            })

        return relationships

    def predict_hierarchy(
        self,
        phrases: List[str],
        threshold: float = None,
        remove_self_loops: bool = True
    ) -> List[Dict[str, str]]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ñ—Ä–∞–∑ (phrases √ó phrases)

        Args:
            phrases: —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–µ—Ä–∞—Ä—Ö–∏–∏
            threshold: –ø–æ—Ä–æ–≥ –¥–ª—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π F1 threshold –∏–∑ –æ–±—É—á–µ–Ω–∏—è)
            remove_self_loops: —É–¥–∞–ª–∏—Ç—å –ª–∏ —Å–∞–º–æ–∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è

        Returns:
            hierarchy: —Å–ø–∏—Å–æ–∫ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"child": "...", "parent": "..."}]
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π F1 threshold –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω —è–≤–Ω–æ
        if threshold is None:
            threshold = getattr(self, 'f1_threshold', None)
            if threshold is None:
                threshold = 0.5

        # –ü–æ–ª—É—á–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä
        attention_matrix = self.forward(phrases, phrases)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥
        binary_matrix = (attention_matrix > threshold).cpu().numpy()

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –Ω–µ–Ω—É–ª–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ)
        nonzero_indices = np.nonzero(binary_matrix)
        i_indices, j_indices = nonzero_indices[0], nonzero_indices[1]

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–Ω–æ—à–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–Ω—É–ª–µ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        hierarchy = []
        for idx in range(len(i_indices)):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∞–º–æ–∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            i, j = i_indices[idx], j_indices[idx]
            if remove_self_loops and i == j:
                    continue

            hierarchy.append({
                    "child": phrases[i],
                    "parent": phrases[j],
                    "confidence": float(attention_matrix[i, j].item())
                })

        return hierarchy

    def save_pretrained(self, save_directory: str):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ (–≤–∫–ª—é—á–∞—è –≤—Å–µ –≤–µ—Å–∞)
        –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç LORA –∞–¥–∞–ø—Ç–µ—Ä—ã, –æ–Ω–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ "—Å–∫–ª–µ–∏–≤–∞—é—Ç—Å—è" –≤ —Ü–µ–ª—å–Ω—ã–µ –≤–µ—Å–∞

        Args:
            save_directory: –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        os.makedirs(save_directory, exist_ok=True)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ LORA –∞–¥–∞–ø—Ç–µ—Ä—ã
        has_lora = self._check_has_lora()

        if has_lora:
            print(f"üîÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã LORA –∞–¥–∞–ø—Ç–µ—Ä—ã, —Å–∫–ª–µ–∏–≤–∞–µ–º –≤ —Ü–µ–ª—å–Ω—ã–µ –≤–µ—Å–∞...")
            # –°–∫–ª–µ–∏–≤–∞–µ–º LORA –∞–¥–∞–ø—Ç–µ—Ä—ã –≤ —Ü–µ–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
            merged_state_dict = self._merge_lora_weights()
        else:
            # –û–±—ã—á–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏
            merged_state_dict = self.state_dict()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ü–µ–ª—å–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        model_state = {
            'model_state_dict': merged_state_dict,
            'qwen3_model_name': self.qwen3_model_name,
            'max_length': self.max_length,
            'embedding_batch_size': self.embedding_batch_size,
            'freeze_qwen3': self.freeze_qwen3,
            'embedding_dim': self.embedding_dim,
            'embedding_pooling': self.embedding_pooling,
            'f1_threshold': getattr(self, 'f1_threshold', None),
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–º–æ—Ä–æ–∑–∫–∏
            "freeze_strategy": getattr(self, 'freeze_strategy', 'unknown'),
            "lora_rank": getattr(self, 'lora_rank', None),
            "lora_alpha": getattr(self, 'lora_alpha', None),
            "has_lora": self._check_has_lora()
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
        torch.save(model_state, os.path.join(save_directory, "model.pt"))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        meta_config = {
            "model_type": "qwen3_cross_attention_meta_model",
            "qwen3_model_name": self.qwen3_model_name,
            "max_length": self.max_length,
            "embedding_batch_size": self.embedding_batch_size,
            "freeze_qwen3": self.freeze_qwen3,
            "embedding_dim": self.embedding_dim,
            "embedding_pooling": self.embedding_pooling,
            "f1_threshold": getattr(self, 'f1_threshold', None),
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–º–æ—Ä–æ–∑–∫–∏
            "freeze_strategy": getattr(self, 'freeze_strategy', 'unknown'),
            "lora_rank": getattr(self, 'lora_rank', None),
            "lora_alpha": getattr(self, 'lora_alpha', None),
            "has_lora": has_lora
        }

        with open(os.path.join(save_directory, "config.json"), 'w') as f:
            json.dump(meta_config, f, indent=2)

        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º CrossAttentionModel –æ—Ç–¥–µ–ª—å–Ω–æ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        cross_attention_dir = os.path.join(
    save_directory, "cross_attention_model")
        self.cross_attention_model.save_pretrained(cross_attention_dir)

        print(f"–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {save_directory}")
        print(f"   - –í–µ—Å–∞ –º–æ–¥–µ–ª–∏: model.pt")
        print(f"   - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: config.json")
        print(f"   - CrossAttention: cross_attention_model/")

    def _check_has_lora(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ LORA –∞–¥–∞–ø—Ç–µ—Ä—ã –≤ –º–æ–¥–µ–ª–∏"""
        try:
            from peft import PeftModel
            return isinstance(self.qwen3_model, PeftModel)
        except BaseException:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            for name, _ in self.qwen3_model.named_parameters():
                if 'lora_A' in name or 'lora_B' in name or 'base_model' in name:
                    return True
            return False

    def _merge_lora_weights(self) -> dict:
        """–°–∫–ª–µ–∏–≤–∞–µ—Ç LORA –∞–¥–∞–ø—Ç–µ—Ä—ã –≤ —Ü–µ–ª—å–Ω—ã–µ –≤–µ—Å–∞"""
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ PEFT
            from peft import PeftModel
            if isinstance(self.qwen3_model, PeftModel):
                print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π merge PEFT...")
                # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –º–æ–¥–µ–ª–∏ –∏ —Å–ª–∏–≤–∞–µ–º LORA
                merged_model = self.qwen3_model.merge_and_unload()

                # –ü–æ–ª—É—á–∞–µ–º state_dict –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
                original_state = self.state_dict()
                merged_qwen3_state = merged_model.state_dict()

                # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏
                final_state = {}

                # –ö–æ–ø–∏—Ä—É–µ–º –≤—Å–µ non-qwen3 –≤–µ—Å–∞ –∫–∞–∫ –µ—Å—Ç—å
                for key, value in original_state.items():
                    if not key.startswith('qwen3_model.'):
                        final_state[key] = value

                # –ó–∞–º–µ–Ω—è–µ–º qwen3 –≤–µ—Å–∞ –Ω–∞ —Å–∫–ª–µ–µ–Ω–Ω—ã–µ
                for merged_key, merged_value in merged_qwen3_state.items():
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å qwen3_model –∫ –∫–ª—é—á—É –∏–∑ merged –º–æ–¥–µ–ª–∏
                    final_key = f"qwen3_model.{merged_key}"
                    final_state[final_key] = merged_value
                    print(f"   –°–∫–ª–µ–µ–Ω –∫–ª—é—á: {merged_key} -> {final_key}")

                print(f"   ‚úÖ LORA –∞–¥–∞–ø—Ç–µ—Ä—ã —É—Å–ø–µ—à–Ω–æ —Å–∫–ª–µ–µ–Ω—ã")
                return final_state

        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–ª–µ–∏–≤–∞–Ω–∏–∏ LORA: {e}")
            print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—á–Ω–æ–µ —Å–∫–ª–µ–∏–≤–∞–Ω–∏–µ...")

        # –†—É—á–Ω–æ–µ —Å–∫–ª–µ–∏–≤–∞–Ω–∏–µ –∫–∞–∫ fallback
        return self._manual_merge_lora()

    def _manual_merge_lora(self) -> dict:
        """–†—É—á–Ω–æ–µ —Å–∫–ª–µ–∏–≤–∞–Ω–∏–µ LORA –≤–µ—Å–æ–≤"""
        state_dict = self.state_dict()
        merged_state = {}

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º LORA —Å–ª–æ–∏
        lora_layers = {}

        for name, param in state_dict.items():
            if 'base_model.model.' in name:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è —Å–ª–æ—è, —É–±–∏—Ä–∞—è base_model.model
                # –ò–∑: qwen3_model.base_model.model.layers.0.self_attn.q_proj.base_layer.weight
                # –í:  qwen3_model.layers.0.self_attn.q_proj
                clean_name = name.replace('.base_model.model.', '.')
                layer_name = clean_name.split('.base_layer.')[
                                              0] if '.base_layer.' in clean_name else None

                if layer_name:
                    if layer_name not in lora_layers:
                        lora_layers[layer_name] = {}

                    if '.base_layer.' in name:
                        lora_layers[layer_name]['base'] = param
                    elif '.lora_A.default.' in name:
                        lora_layers[layer_name]['lora_A'] = param
                    elif '.lora_B.default.' in name:
                        lora_layers[layer_name]['lora_B'] = param
            else:
                # –û–±—ã—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Å—Ç–∞—é—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å
                merged_state[name] = param

        # –°–∫–ª–µ–∏–≤–∞–µ–º LORA —Å–ª–æ–∏
        lora_alpha = getattr(self, 'lora_alpha', 16)
        lora_rank = getattr(self, 'lora_rank', 8)
        scaling = lora_alpha / lora_rank

        for layer_name, lora_parts in lora_layers.items():
            if all(key in lora_parts for key in ['base', 'lora_A', 'lora_B']):
                # –°–∫–ª–µ–∏–≤–∞–µ–º: W_final = W_base + lora_B @ lora_A * scaling
                base_weight = lora_parts['base']
                lora_A = lora_parts['lora_A']
                lora_B = lora_parts['lora_B']

                # –í—ã—á–∏—Å–ª—è–µ–º –¥–µ–ª—å—Ç—É LORA
                delta = torch.mm(lora_B, lora_A) * scaling

                # –§–∏–Ω–∞–ª—å–Ω—ã–π –≤–µ—Å
                final_weight = base_weight + delta

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–¥ –æ–±—ã—á–Ω—ã–º –∏–º–µ–Ω–µ–º (–±–µ–∑ .base_layer)
                final_name = layer_name + '.weight'
                merged_state[final_name] = final_weight

                print(f"   –°–∫–ª–µ–µ–Ω —Å–ª–æ–π: {layer_name}")
            else:
                # –ï—Å–ª–∏ –Ω–µ –≤—Å–µ —á–∞—Å—Ç–∏ LORA –Ω–∞–π–¥–µ–Ω—ã, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑–æ–≤—ã–π –≤–µ—Å
                if 'base' in lora_parts:
                    final_name = layer_name + '.weight'
                    merged_state[final_name] = lora_parts['base']

        print(f"   ‚úÖ –†—É—á–Ω–æ–µ —Å–∫–ª–µ–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        return merged_state

    @classmethod
    def from_pretrained(
        cls,
        model_path: str,
        device: str = "auto",
        **kwargs
    ):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è

        Args:
            model_path: –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
            device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
            **kwargs: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞

        Returns:
            meta_model: —ç–∫–∑–µ–º–ø–ª—è—Ä Qwen3CrossAttentionMetaModel
        """
        model_path = Path(model_path)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        if device == "auto":
            device = "cuda" if torch.cuda.is_available() else "cpu"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ —Å –≤–µ—Å–∞–º–∏
        model_pt_path = model_path / "model.pt"
        config_path = model_path / "config.json"

        if model_pt_path.exists():
            # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç - –∑–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—É—é –º–æ–¥–µ–ª—å
            print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ {model_pt_path}")
            model_state = torch.load(model_pt_path, map_location=device)

            # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å CrossAttentionModel –∏–∑ –ø–∞–ø–∫–∏ (–µ—Å–ª–∏
            # –µ—Å—Ç—å)
            cross_attention_path = model_path / "cross_attention_model"
            cross_attention_model = None

            if cross_attention_path.exists():
                try:
                    from cross_attention_model import CrossAttentionModel
                    cross_attention_model = CrossAttentionModel.from_pretrained(
                        str(cross_attention_path))
                    print(f"‚úÖ CrossAttentionModel –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –ø–∞–ø–∫–∏")
                except Exception as e:
                    print(
                        f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CrossAttentionModel –∏–∑ –ø–∞–ø–∫–∏: {e}")

            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞-–º–æ–¥–µ–ª—å —Å —Ç–µ–º–∏ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            meta_model = cls(
    cross_attention_model=cross_attention_model,
    qwen3_model_name=model_state["qwen3_model_name"],
    max_length=model_state["max_length"],
    embedding_batch_size=model_state["embedding_batch_size"],
    freeze_qwen3=model_state["freeze_qwen3"],
    embedding_pooling=model_state.get(
        "embedding_pooling",
        "last_token_norm"),
        device=device,
         **kwargs)

            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–º–æ—Ä–æ–∑–∫–∏ (–¥–ª—è —Å–ø—Ä–∞–≤–∫–∏)
            if 'freeze_strategy' in model_state:
                meta_model.freeze_strategy = model_state['freeze_strategy']
                meta_model.lora_rank = model_state.get('lora_rank')
                meta_model.lora_alpha = model_state.get('lora_alpha')

                if model_state.get('has_lora', False):
                    print(
                        f"‚úÖ –ú–æ–¥–µ–ª—å –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ —Å LORA (rank={meta_model.lora_rank}, alpha={meta_model.lora_alpha})")
                    print(f"   LORA –∞–¥–∞–ø—Ç–µ—Ä—ã —É–∂–µ —Å–∫–ª–µ–µ–Ω—ã –≤ —Ü–µ–ª—å–Ω—ã–µ –≤–µ—Å–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∫–ª–µ–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ (–¥–æ–ª–∂–Ω—ã —Ç–æ—á–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å)
            try:
                meta_model.load_state_dict(model_state['model_state_dict'])
                print(f"‚úÖ –í–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ—Å–æ–≤: {e}")
                print(f"   –í–æ–∑–º–æ–∂–Ω–æ, –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã")
                # –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–ª—å–∫–æ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –≤–µ—Å–∞
                current_state = meta_model.state_dict()
                compatible_state = {}
                for key, value in model_state['model_state_dict'].items():
                    if key in current_state and current_state[key].shape == value.shape:
                        compatible_state[key] = value
                    else:
                        print(f"   –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å–ª–æ–π: {key}")

                meta_model.load_state_dict(compatible_state, strict=False)
                print(f"‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –≤–µ—Å–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º f1_threshold
            if model_state.get("f1_threshold") is not None:
                meta_model.f1_threshold = model_state["f1_threshold"]
                print(
                    f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω F1 threshold: {meta_model.f1_threshold:.4f}")

            print(f"‚úÖ –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path} (–ø–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å)")
            return meta_model

        elif config_path.exists():
            # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç - –∑–∞–≥—Ä—É–∂–∞–µ–º –ø–æ —á–∞—Å—Ç—è–º (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ —Å—Ç–∞—Ä–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –∏–∑ {config_path}")
            with open(config_path, 'r') as f:
                meta_config = json.load(f)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º CrossAttentionModel
            cross_attention_path = model_path / "cross_attention_model"
            if not cross_attention_path.exists():
                raise FileNotFoundError(
                    f"cross_attention_model –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ {model_path}")

            cross_attention_model = CrossAttentionModel.from_pretrained(
                str(cross_attention_path))

            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞-–º–æ–¥–µ–ª—å
            meta_model = cls(
                cross_attention_model=cross_attention_model,
                qwen3_model_name=meta_config["qwen3_model_name"],
                max_length=meta_config["max_length"],
                embedding_batch_size=meta_config["embedding_batch_size"],
                freeze_qwen3=meta_config["freeze_qwen3"],
                embedding_pooling=meta_config.get("embedding_pooling", "last_token_norm"),
                device=device,
                **kwargs
            )

            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º f1_threshold –µ—Å–ª–∏ –µ—Å—Ç—å
            if meta_config.get("f1_threshold") is not None:
                meta_model.f1_threshold = meta_config["f1_threshold"]
                print(
                    f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω F1 threshold: {meta_model.f1_threshold:.4f}")

            print(f"‚ö†Ô∏è  –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path} (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç)")
            print(f"   –í–Ω–∏–º–∞–Ω–∏–µ: –≤–µ—Å–∞ Qwen3 –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ –æ–±—É—á–µ–Ω—ã!")
            return meta_model

        else:
            raise FileNotFoundError(
                f"–ù–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ model.pt, –Ω–∏ config.json –≤ {model_path}")

    @classmethod
    def from_cross_attention_checkpoint(
        cls,
        cross_attention_results_dir: str,
        qwen3_model_name: str = "Qwen/Qwen3-Embedding-4B",
        device: str = "auto",
        **kwargs
    ):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è CrossAttentionModel

        Args:
            cross_attention_results_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
            qwen3_model_name: –∏–º—è –º–æ–¥–µ–ª–∏ Qwen3
            device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
            **kwargs: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞

        Returns:
            meta_model: —ç–∫–∑–µ–º–ø–ª—è—Ä Qwen3CrossAttentionMetaModel
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é CrossAttentionModel
        from inference_cross_attention import load_trained_model

        cross_attention_model, best_results, f1_threshold = load_trained_model(
            cross_attention_results_dir
        )

        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞-–º–æ–¥–µ–ª—å
        meta_model = cls(
            cross_attention_model=cross_attention_model,
            qwen3_model_name=qwen3_model_name,
            device=device,
            **kwargs
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        meta_model.best_results = best_results
        meta_model.f1_threshold = f1_threshold

        print(f"–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ –∏–∑ {cross_attention_results_dir}")
        print(f"–õ—É—á—à–∏–π F1 –ø–æ—Ä–æ–≥: {f1_threshold:.4f}")

        return meta_model

    def get_model_info(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏

        Returns:
            info: —Å–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª–∏
        """
        cross_attention_info = self.cross_attention_model.get_model_info()

        qwen3_params = sum(p.numel() for p in self.qwen3_model.parameters())
        cross_attention_params = cross_attention_info['num_trainable_params']

        info = {
            'model_type': 'Qwen3CrossAttentionMetaModel',
            'qwen3_model_name': self.qwen3_model_name,
            'embedding_dim': self.embedding_dim,
            'max_length': self.max_length,
            'embedding_batch_size': self.embedding_batch_size,
            'freeze_qwen3': self.freeze_qwen3,
            'embedding_pooling': self.embedding_pooling,
            'device': self.device,
            'f1_threshold': getattr(self, 'f1_threshold', None),
            'qwen3_params': qwen3_params,
            'cross_attention_params': cross_attention_params,
            'total_params': qwen3_params + cross_attention_params,
            'cross_attention_model_info': cross_attention_info
        }

        return info

    def __repr__(self):
        info = self.get_model_info()
        threshold_str = f", threshold={info['f1_threshold']:.4f}" if info['f1_threshold'] is not None else ""
        return f"Qwen3CrossAttentionMetaModel(" \
               f"qwen3={info['qwen3_model_name']}, " \
               f"embedding_dim={info['embedding_dim']}, " \
               f"pooling={info['embedding_pooling']}, " \
               f"freeze_qwen3={info['freeze_qwen3']}" \
               f"{threshold_str}, " \
               f"total_params={info['total_params']:,})"


def create_meta_model_from_checkpoint(
    cross_attention_results_dir: str,
    qwen3_model_name: str = "Qwen/Qwen3-Embedding-4B",
    device: str = "auto",
    **kwargs
) -> Qwen3CrossAttentionMetaModel:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞

    Args:
        cross_attention_results_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
        qwen3_model_name: –∏–º—è –º–æ–¥–µ–ª–∏ Qwen3
        device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        **kwargs: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã

    Returns:
        meta_model: —ç–∫–∑–µ–º–ø–ª—è—Ä Qwen3CrossAttentionMetaModel
    """
    return Qwen3CrossAttentionMetaModel.from_cross_attention_checkpoint(
        cross_attention_results_dir=cross_attention_results_dir,
        qwen3_model_name=qwen3_model_name,
        device=device,
        **kwargs
    )


def create_meta_model_from_scratch(
    qwen3_model_name: str = "Qwen/Qwen3-Embedding-0.6B",
    num_attention_heads: int = 8,
    num_key_value_heads: int = 8,
    layer_idx: int = 0,
    device: str = "auto",
    init_from_qwen3: bool = True,
    embedding_pooling: str = "mean",
    **kwargs
) -> Qwen3CrossAttentionMetaModel:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Qwen3

    Args:
        qwen3_model_name: –∏–º—è –º–æ–¥–µ–ª–∏ Qwen3 –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        num_attention_heads: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
        num_key_value_heads: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –¥–ª—è key-value (GQA)
        layer_idx: –∏–Ω–¥–µ–∫—Å —Å–ª–æ—è –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ (–µ—Å–ª–∏ init_from_qwen3=True)
        device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        init_from_qwen3: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ª–∏ CrossAttentionModel –∏–∑ –≤–µ—Å–æ–≤ Qwen3
        embedding_pooling: –º–µ—Ç–æ–¥ pooling –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ("mean", "last_token", "last_token_norm")
        **kwargs: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏

    Returns:
        meta_model: —ç–∫–∑–µ–º–ø–ª—è—Ä Qwen3CrossAttentionMetaModel
    """
    from transformers import AutoModel, AutoConfig

    print(f"üöÄ –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ {qwen3_model_name}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é Qwen3 –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    print("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Qwen3...")
    qwen3_config = AutoConfig.from_pretrained(qwen3_model_name)
    hidden_size = qwen3_config.hidden_size

    print(f"   - –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {hidden_size}")
    print(f"   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è: {num_attention_heads}")
    print(f"   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ KV –≥–æ–ª–æ–≤: {num_key_value_heads}")

    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è CrossAttentionModel
    cross_attention_config = {
        'hidden_size': hidden_size,
        'num_attention_heads': num_attention_heads,
        'num_key_value_heads': num_key_value_heads,
        'rms_norm_eps': getattr(qwen3_config, 'rms_norm_eps', 1e-6),
        'attention_bias': getattr(qwen3_config, 'attention_bias', False),
        'head_dim': hidden_size // num_attention_heads
    }

    # –°–æ–∑–¥–∞–µ–º CrossAttentionModel
    print("üîß –°–æ–∑–¥–∞–Ω–∏–µ CrossAttentionModel...")
    cross_attention_model = CrossAttentionModel(
    cross_attention_config, layer_idx=layer_idx)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑ –≤–µ—Å–æ–≤ Qwen3 –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if init_from_qwen3:
        print("‚ö° –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑ –≤–µ—Å–æ–≤ Qwen3...")
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Qwen3 –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤
            qwen3_model = AutoModel.from_pretrained(qwen3_model_name)

            # –ü–æ–ª—É—á–∞–µ–º —Å–ª–æ–π –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            if hasattr(
    qwen3_model, 'layers') and len(
        qwen3_model.layers) > layer_idx:
                qwen3_layer = qwen3_model.layers[layer_idx]

                # –ö–æ–ø–∏—Ä—É–µ–º –≤–µ—Å–∞ attention
                if hasattr(qwen3_layer, 'self_attn'):
                    qwen3_attn = qwen3_layer.self_attn

                    # –ö–æ–ø–∏—Ä—É–µ–º q_proj –∏ k_proj
                    if hasattr(qwen3_attn, 'q_proj'):
                        cross_attention_model.q_proj.weight.data.copy_(
                            qwen3_attn.q_proj.weight.data)
                        if cross_attention_model.q_proj.bias is not None and qwen3_attn.q_proj.bias is not None:
                            cross_attention_model.q_proj.bias.data.copy_(
                                qwen3_attn.q_proj.bias.data)

                    if hasattr(qwen3_attn, 'k_proj'):
                        cross_attention_model.k_proj.weight.data.copy_(
                            qwen3_attn.k_proj.weight.data)
                        if cross_attention_model.k_proj.bias is not None and qwen3_attn.k_proj.bias is not None:
                            cross_attention_model.k_proj.bias.data.copy_(
                                qwen3_attn.k_proj.bias.data)

                    print("   ‚úÖ –í–µ—Å–∞ q_proj –∏ k_proj —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω—ã")
                    cross_attention_model.initialized_from_qwen3 = True
                else:
                    print("   ‚ö†Ô∏è self_attn –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–ª–æ–µ Qwen3")
            else:
                print(f"   ‚ö†Ô∏è –°–ª–æ–π {layer_idx} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–æ–¥–µ–ª–∏ Qwen3")

            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å
            del qwen3_model
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–∑ Qwen3: {e}")
            print("   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–ª—É—á–∞–π–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è")

    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞-–º–æ–¥–µ–ª—å
    print("üèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏...")
    meta_model = Qwen3CrossAttentionMetaModel(
        cross_attention_model=cross_attention_model,
        qwen3_model_name=qwen3_model_name,
        device=device,
        embedding_pooling=embedding_pooling,
        **kwargs
    )

    # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: f1_threshold –æ—Å—Ç–∞–µ—Ç—Å—è None –¥–ª—è –º–æ–¥–µ–ª–µ–π, —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Å –Ω—É–ª—è
    # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ –º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –≤—Ä—É—á–Ω—É—é

    print("‚úÖ –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    print(
        f"   - –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {meta_model.get_model_info()['total_params']:,}")
    print(
        f"   - F1 threshold: {meta_model.f1_threshold} (–±—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è)")

    return meta_model


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Qwen3CrossAttentionMetaModel...")

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é CrossAttentionModel
    test_config = {
        'hidden_size': 2560,  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å Qwen3-Embedding-4B
        'num_attention_heads': 8,
        'num_key_value_heads': 8,
        'rms_norm_eps': 1e-6,
        'attention_bias': False
    }

    cross_attention_model = CrossAttentionModel(test_config)

    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞-–º–æ–¥–µ–ª—å
    meta_model = Qwen3CrossAttentionMetaModel(
        cross_attention_model=cross_attention_model,
        embedding_batch_size=2,  # –ú–∞–ª–µ–Ω—å–∫–∏–π –±–∞—Ç—á –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        device="cpu"  # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    )

    print(f"–°–æ–∑–¥–∞–Ω–∞ –º–µ—Ç–∞-–º–æ–¥–µ–ª—å: {meta_model}")

    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    phrases_1 = ["machine learning", "deep learning", "neural networks"]
    phrases_2 = ["artificial intelligence", "computer science", "technology"]

    print(f"\n–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ:")
    print(f"  phrases_1: {phrases_1}")
    print(f"  phrases_2: {phrases_2}")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º forward pass
    with torch.no_grad():
        attention_matrix = meta_model(phrases_1, phrases_2)

    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"  –ú–∞—Ç—Ä–∏—Ü–∞ –≤–Ω–∏–º–∞–Ω–∏—è: {attention_matrix.shape}")
    print(f"  –ú–∞—Ç—Ä–∏—Ü–∞ –≤–Ω–∏–º–∞–Ω–∏—è:\n{attention_matrix}")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–π (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π threshold)
    relationships = meta_model.predict_relationships(phrases_1, phrases_2)
    current_threshold = getattr(meta_model, 'f1_threshold', 0.5)
    print(f"\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è (threshold={current_threshold}):")
    for rel in relationships:
        print(
            f"  {rel['child']} -> {rel['parent']} (confidence: {rel['confidence']:.4f})")

    # –¢–∞–∫–∂–µ —Ç–µ—Å—Ç–∏—Ä—É–µ–º —Å —è–≤–Ω—ã–º threshold
    relationships_custom = meta_model.predict_relationships(
        phrases_1, phrases_2, threshold=0.3)
    print(f"\n–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è (—è–≤–Ω—ã–π threshold=0.3):")
    for rel in relationships_custom:
        print(
            f"  {rel['child']} -> {rel['parent']} (confidence: {rel['confidence']:.4f})")

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
    info = meta_model.get_model_info()
    print(f"\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏:")
    for key, value in info.items():
        if key != 'cross_attention_model_info':
            print(f"  {key}: {value}")

    print("\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
