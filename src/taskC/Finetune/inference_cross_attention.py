"""
–°–∫—Ä–∏–ø—Ç –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª—è Cross-Attention –º–æ–¥–µ–ª–∏
–ü—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–µ—Ä–∞—Ä—Ö–∏–π —Ç–µ—Ä–º–∏–Ω–æ–≤
"""

import os
import json
import numpy as np
import torch
from typing import List, Dict, Tuple, Optional
from transformers import AutoTokenizer, AutoModel
import argparse
from pathlib import Path
import logging
from datetime import datetime
import tqdm
from sklearn.metrics import precision_recall_fscore_support
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')

# –ò–º–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏ –∏ —É—Ç–∏–ª–∏—Ç
from cross_attention_model import CrossAttentionModel


def setup_logging(log_file: str = None) -> logging.Logger:
    """
    –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    
    Args:
        log_file: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –ª–æ–≥–∞
        
    Returns:
        logger: –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –ª–æ–≥–≥–µ—Ä
    """
    logger = logging.getLogger("cross_attention_inference")
    logger.setLevel(logging.INFO)
    
    # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ handlers
    logger.handlers.clear()
    
    # –§–æ—Ä–º–∞—Ç—Ç–µ—Ä –¥–ª—è –ª–æ–≥–æ–≤
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Handler –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # Handler –¥–ª—è —Ñ–∞–π–ª–∞ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
    if log_file:
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger


def load_embedding_model():
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    
    Returns:
        model, tokenizer: –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    """
    model_name = "Qwen/Qwen3-Embedding-4B"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    
    if torch.cuda.is_available():
        model = model.cuda()
    model.eval()
    
    return model, tokenizer


def get_term_embeddings(terms: List[str], model, tokenizer, batch_size: int = 32) -> List[List[float]]:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤
    
    Args:
        terms: —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤
        model: –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        tokenizer: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        
    Returns:
        embeddings: —Å–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    """
    embeddings = []
    
    with torch.no_grad():
        for i in tqdm.tqdm(range(0, len(terms), batch_size), desc="Generating embeddings"):
            batch_terms = terms[i:i+batch_size]
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–∞
            inputs = tokenizer(
                batch_terms, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=512
            )
            
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            outputs = model(**inputs)
            batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()
            
            embeddings.extend(batch_embeddings.tolist())
    
    return embeddings


def load_trained_model(results_dir: str) -> Tuple[CrossAttentionModel, Dict, float]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
    
    Args:
        results_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
        
    Returns:
        model: –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        best_results: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        f1_threshold: –ª—É—á—à–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è F1 score
    """
    results_path = Path(results_dir)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
    best_results_file = results_path / "best_results.json"
    if not best_results_file.exists():
        raise FileNotFoundError(f"best_results.json not found in {results_dir}")
    
    with open(best_results_file, 'r') as f:
        best_results = json.load(f)
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–≥–æ –ø–æ—Ä–æ–≥–∞ –¥–ª—è F1
    f1_threshold = best_results['best_results']['best_thresholds']['f1_score']['threshold']
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª—è—Ö
    best_models_file = results_path / "best_models" / "best_models.json"
    if not best_models_file.exists():
        raise FileNotFoundError(f"best_models.json not found in {results_dir}/best_models/")
    
    with open(best_models_file, 'r') as f:
        best_models_info = json.load(f)
    
    # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å (–ø–µ—Ä–≤—É—é –≤ —Å–ø–∏—Å–∫–µ)
    best_model_info = best_models_info['best_models'][0]
    model_path = best_model_info['model_path']
    
    if not os.path.exists(model_path):
        # –ü—Ä–æ–±—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
        model_filename = best_model_info['model_filename']
        model_path = results_path / "best_models" / model_filename
        
        if not model_path.exists():
            raise FileNotFoundError(f"Model file not found: {model_path}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π PyTorch 2.6
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ
        checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)
    except Exception:
        # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–± (–¥–ª—è —Å—Ç–∞—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π)
        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    config = checkpoint['config']
    # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ config - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å
    if not isinstance(config, dict):
        if hasattr(config, 'to_dict'):
            config = config.to_dict()
        else:
            # –ï—Å–ª–∏ —ç—Ç–æ –æ–±—ä–µ–∫—Ç, –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –µ–≥–æ –∞—Ç—Ä–∏–±—É—Ç—ã –≤ —Å–ª–æ–≤–∞—Ä—å
            config = {k: v for k, v in config.__dict__.items() 
                     if not k.startswith('_')}
    
    model = CrossAttentionModel(config, layer_idx=checkpoint.get('layer_idx', 0))
    model.load_state_dict(checkpoint['model_state_dict'])
    
    if torch.cuda.is_available():
        model = model.cuda()
    
    model.eval()
    
    return model, best_results, f1_threshold


def build_prediction_matrix(
    embeddings: List[List[float]], 
    model: CrossAttentionModel, 
    batch_size: int = 64,
    logger: logging.Logger = None
) -> np.ndarray:
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π terms x terms
    
    Args:
        embeddings: —Å–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ—Ä–º–∏–Ω–æ–≤
        model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        logger: –ª–æ–≥–≥–µ—Ä –¥–ª—è –≤—ã–≤–æ–¥–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        
    Returns:
        pred_matrix: –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Ä–∞–∑–º–µ—Ä–∞ (n_terms, n_terms)
    """
    n_terms = len(embeddings)
    pred_matrix = np.zeros((n_terms, n_terms), dtype=np.float32)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤ —Ç–µ–Ω–∑–æ—Ä
    embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)
    if torch.cuda.is_available():
        embeddings_tensor = embeddings_tensor.cuda()
    
    if logger:
        logger.info(f"–°—Ç—Ä–æ–∏–º –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π {n_terms}x{n_terms}")
    
    with torch.no_grad():
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ –±–ª–æ–∫–∞–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        for i in tqdm.tqdm(range(0, n_terms, batch_size), desc="Building prediction matrix"):
            end_i = min(i + batch_size, n_terms)
            batch_embeddings_1 = embeddings_tensor[i:end_i]  # (batch_size, embedding_dim)
            
            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞ —Å—Ç—Ä–æ–∫ –≤—ã—á–∏—Å–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ –≤—Å–µ–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏
            for j in range(0, n_terms, batch_size):
                end_j = min(j + batch_size, n_terms)
                batch_embeddings_2 = embeddings_tensor[j:end_j]  # (batch_size, embedding_dim)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –±–ª–æ–∫–∞
                # batch_embeddings_1: (batch_i, embedding_dim)
                # batch_embeddings_2: (batch_j, embedding_dim)
                # –ù—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å: (batch_i, batch_j)
                
                batch_pred = model(batch_embeddings_1, batch_embeddings_2)
                pred_matrix[i:end_i, j:end_j] = batch_pred.cpu().numpy()
    
    return pred_matrix


def apply_threshold_and_extract_pairs(
    pred_matrix: np.ndarray,
    terms: List[str],
    threshold: float,
    logger: logging.Logger = None
) -> List[Dict[str, str]]:
    """
    –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ –∫ –º–∞—Ç—Ä–∏—Ü–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä child-parent
    
    Args:
        pred_matrix: –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        terms: —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤
        threshold: –ø–æ—Ä–æ–≥ –¥–ª—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏
        logger: –ª–æ–≥–≥–µ—Ä
        
    Returns:
        pairs: —Å–ø–∏—Å–æ–∫ –ø–∞—Ä –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"parent": "...", "child": "..."}]
    """
    pairs = []
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥
    binary_matrix = (pred_matrix > threshold).astype(int)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä—ã (i, j) –≥–¥–µ binary_matrix[i, j] = 1
    # i - –∏–Ω–¥–µ–∫—Å child, j - –∏–Ω–¥–µ–∫—Å parent
    child_indices, parent_indices = np.where(binary_matrix == 1)
    
    for child_idx, parent_idx in zip(child_indices, parent_indices):
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∞–º–æ—Å–≤—è–∑–∏
        if child_idx == parent_idx:
            continue
            
        pairs.append({
            "parent": terms[parent_idx],
            "child": terms[child_idx]
        })
    
    if logger:
        logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(pairs)} –ø–∞—Ä —Å –ø–æ—Ä–æ–≥–æ–º {threshold}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–∞—Ç—Ä–∏—Ü–µ
        total_pairs = len(terms) * (len(terms) - 1)  # –ë–µ–∑ –¥–∏–∞–≥–æ–Ω–∞–ª–∏
        positive_pairs = len(pairs)
        logger.info(f"–í—Å–µ–≥–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø–∞—Ä: {total_pairs}")
        logger.info(f"–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä: {positive_pairs} ({positive_pairs/total_pairs*100:.2f}%)")
    
    return pairs


def save_results(
    pred_matrix: np.ndarray,
    pairs: List[Dict[str, str]],
    terms: List[str],
    output_dir: str,
    threshold: float,
    best_results: Dict,
    logger: logging.Logger = None
):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    
    Args:
        pred_matrix: –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        pairs: –ø–∞—Ä—ã child-parent
        terms: —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤
        output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        threshold: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥
        best_results: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        logger: –ª–æ–≥–≥–µ—Ä
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    matrix_file = output_path / "prediction_matrix.npy"
    np.save(matrix_file, pred_matrix)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞—Ä –≤ JSON
    pairs_file = output_path / "predicted_pairs.json"
    with open(pairs_file, 'w', encoding='utf-8') as f:
        json.dump(pairs, f, ensure_ascii=False, indent=2)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤
    terms_file = output_path / "terms.json"
    with open(terms_file, 'w', encoding='utf-8') as f:
        json.dump(terms, f, ensure_ascii=False, indent=2)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    metadata = {
        "inference_info": {
            "timestamp": datetime.now().isoformat(),
            "n_terms": len(terms),
            "n_pairs": len(pairs),
            "threshold_used": threshold,
            "matrix_shape": pred_matrix.shape,
            "matrix_stats": {
                "min": float(pred_matrix.min()),
                "max": float(pred_matrix.max()),
                "mean": float(pred_matrix.mean()),
                "std": float(pred_matrix.std())
            }
        },
        "model_info": {
            "best_f1_threshold": best_results['best_results']['best_thresholds']['f1_score']['threshold'],
            "best_f1_value": best_results['best_results']['best_thresholds']['f1_score']['value'],
            "model_roc_auc": best_results['best_results']['roc_auc'],
            "dataset": best_results['experiment_info']['dataset']
        }
    }
    
    metadata_file = output_path / "inference_metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    plot_prediction_distribution(pred_matrix, threshold, output_path)
    
    if logger:
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_dir}")
        logger.info(f"  üìä –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {matrix_file}")
        logger.info(f"  üîó –ü–∞—Ä—ã: {pairs_file}")
        logger.info(f"  üìù –¢–µ—Ä–º–∏–Ω—ã: {terms_file}")
        logger.info(f"  üìã –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {metadata_file}")


def plot_prediction_distribution(pred_matrix: np.ndarray, threshold: float, output_dir: Path):
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    
    Args:
        pred_matrix: –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        threshold: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥
        output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    plt.figure(figsize=(12, 8))
    
    # –£–±–∏—Ä–∞–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    mask = np.eye(pred_matrix.shape[0], dtype=bool)
    off_diagonal = pred_matrix[~mask]
    
    # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    plt.subplot(2, 2, 1)
    plt.hist(off_diagonal, bins=50, alpha=0.7, edgecolor='black')
    plt.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold:.3f}')
    plt.xlabel('Prediction Score')
    plt.ylabel('Frequency')
    plt.title('Distribution of Prediction Scores')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞
    plt.subplot(2, 2, 2)
    plt.hist(off_diagonal, bins=50, alpha=0.7, edgecolor='black')
    plt.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold:.3f}')
    plt.xlabel('Prediction Score')
    plt.ylabel('Frequency')
    plt.title('Distribution of Prediction Scores (Log Scale)')
    plt.yscale('log')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # –ö—É–º—É–ª—è—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
    plt.subplot(2, 2, 3)
    sorted_scores = np.sort(off_diagonal)
    y_vals = np.arange(1, len(sorted_scores) + 1) / len(sorted_scores)
    plt.plot(sorted_scores, y_vals, linewidth=2)
    plt.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold:.3f}')
    plt.xlabel('Prediction Score')
    plt.ylabel('Cumulative Probability')
    plt.title('Cumulative Distribution of Prediction Scores')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    plt.subplot(2, 2, 4)
    stats_text = f"""
    Matrix Statistics:
    
    Shape: {pred_matrix.shape}
    Total pairs: {pred_matrix.shape[0] * (pred_matrix.shape[1] - 1)}
    
    Score Distribution:
    Min: {off_diagonal.min():.4f}
    Max: {off_diagonal.max():.4f}
    Mean: {off_diagonal.mean():.4f}
    Std: {off_diagonal.std():.4f}
    
    Threshold: {threshold:.3f}
    Above threshold: {np.sum(off_diagonal > threshold)} ({np.sum(off_diagonal > threshold)/len(off_diagonal)*100:.2f}%)
    Below threshold: {np.sum(off_diagonal <= threshold)} ({np.sum(off_diagonal <= threshold)/len(off_diagonal)*100:.2f}%)
    """
    
    plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, 
             verticalalignment='top', fontsize=10, fontfamily='monospace')
    plt.axis('off')
    
    plt.tight_layout()
    plot_path = output_dir / 'prediction_distribution.png'
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {plot_path}")


def load_existing_inference_results(results_dir: str) -> Tuple[np.ndarray, List[str], Dict, float]:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —É–∂–µ –≥–æ—Ç–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    
    Args:
        results_dir: –ø–∞–ø–∫–∞ —Å –≥–æ—Ç–æ–≤—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        
    Returns:
        pred_matrix: –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        terms: —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤
        metadata: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        original_threshold: –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥
    """
    results_path = Path(results_dir)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–∞—Ç—Ä–∏—Ü—ã
    matrix_file = results_path / "prediction_matrix.npy"
    if not matrix_file.exists():
        raise FileNotFoundError(f"–ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {matrix_file}")
    
    pred_matrix = np.load(matrix_file)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤
    terms_file = results_path / "terms.json"
    if not terms_file.exists():
        raise FileNotFoundError(f"–§–∞–π–ª —Å —Ç–µ—Ä–º–∏–Ω–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {terms_file}")
    
    with open(terms_file, 'r', encoding='utf-8') as f:
        terms = json.load(f)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    metadata_file = results_path / "inference_metadata.json"
    if not metadata_file.exists():
        raise FileNotFoundError(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {metadata_file}")
    
    with open(metadata_file, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    original_threshold = metadata['inference_info']['threshold_used']
    
    return pred_matrix, terms, metadata, original_threshold


def run_threshold_experiment(
    existing_results_dir: str,
    new_threshold: float,
    output_dir: str,
    output_suffix: str = None,
    log_file: str = None
) -> Dict:
    """
    –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ—Ä–æ–≥–∞ –∫ —É–∂–µ –≥–æ—Ç–æ–≤–æ–π –º–∞—Ç—Ä–∏—Ü–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    
    Args:
        existing_results_dir: –ø–∞–ø–∫–∞ —Å –≥–æ—Ç–æ–≤—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        new_threshold: –Ω–æ–≤—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä
        output_dir: –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        output_suffix: —Å—É—Ñ—Ñ–∏–∫—Å –¥–ª—è –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "_threshold_05")
        log_file: —Ñ–∞–π–ª –ª–æ–≥–∞
        
    Returns:
        results: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger = setup_logging(log_file)
    
    logger.info("üéØ –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —Å –Ω–æ–≤—ã–º –ø–æ—Ä–æ–≥–æ–º")
    logger.info(f"üìÅ –ì–æ—Ç–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {existing_results_dir}")
    logger.info(f"üéöÔ∏è –ù–æ–≤—ã–π –ø–æ—Ä–æ–≥: {new_threshold}")
    logger.info(f"üíæ –í—ã—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: {output_dir}")
    
    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –≥–æ—Ç–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        logger.info("üìñ –ó–∞–≥—Ä—É–∑–∫–∞ –≥–æ—Ç–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        pred_matrix, terms, metadata, original_threshold = load_existing_inference_results(existing_results_dir)
        
        logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã:")
        logger.info(f"   –ú–∞—Ç—Ä–∏—Ü–∞: {pred_matrix.shape}")
        logger.info(f"   –¢–µ—Ä–º–∏–Ω—ã: {len(terms)}")
        logger.info(f"   –ò—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä–æ–≥: {original_threshold:.3f}")
        logger.info(f"   –ù–æ–≤—ã–π –ø–æ—Ä–æ–≥: {new_threshold:.3f}")
        
        # 2. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ—Ä–æ–≥–∞
        logger.info("üéØ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–æ—Ä–æ–≥–∞...")
        pairs = apply_threshold_and_extract_pairs(pred_matrix, terms, new_threshold, logger)
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        updated_metadata = metadata.copy()
        updated_metadata['threshold_experiment'] = {
            'original_threshold': original_threshold,
            'new_threshold': new_threshold,
            'original_results_dir': existing_results_dir,
            'experiment_timestamp': datetime.now().isoformat()
        }
        updated_metadata['inference_info']['threshold_used'] = new_threshold
        
        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –Ω–æ–≤—ã–º –ø–æ—Ä–æ–≥–æ–º
        logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –Ω–æ–≤—ã–º –ø–æ—Ä–æ–≥–æ–º...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—É—Ñ—Ñ–∏–∫—Å –¥–ª—è —Ñ–∞–π–ª–æ–≤
        if output_suffix is None:
            output_suffix = f"_threshold_{new_threshold:.3f}".replace(".", "")
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞—Ä
        pairs_file = output_path / f"predicted_pairs{output_suffix}.json"
        with open(pairs_file, 'w', encoding='utf-8') as f:
            json.dump(pairs, f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ (–∫–æ–ø–∏—Ä—É–µ–º)
        terms_file = output_path / f"terms{output_suffix}.json"
        with open(terms_file, 'w', encoding='utf-8') as f:
            json.dump(terms, f, ensure_ascii=False, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã (—Å–æ–∑–¥–∞–µ–º —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫—É—é —Å—Å—ã–ª–∫—É –∏–ª–∏ –∫–æ–ø–∏—Ä—É–µ–º)
        matrix_file = output_path / f"prediction_matrix{output_suffix}.npy"
        if not matrix_file.exists():
            original_matrix_file = Path(existing_results_dir) / "prediction_matrix.npy"
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫—É—é —Å—Å—ã–ª–∫—É (—ç–∫–æ–Ω–æ–º–∏—Ç –º–µ—Å—Ç–æ)
                matrix_file.symlink_to(original_matrix_file.absolute())
                logger.info(f"   –°–æ–∑–¥–∞–Ω–∞ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –º–∞—Ç—Ä–∏—Ü—É: {matrix_file}")
            except (OSError, NotImplementedError):
                # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç—Å—è, –∫–æ–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª
                import shutil
                shutil.copy2(original_matrix_file, matrix_file)
                logger.info(f"   –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞: {matrix_file}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        metadata_file = output_path / f"inference_metadata{output_suffix}.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(updated_metadata, f, ensure_ascii=False, indent=2)
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å –Ω–æ–≤—ã–º –ø–æ—Ä–æ–≥–æ–º
        plot_prediction_distribution(pred_matrix, new_threshold, output_path)
        
        logger.info("‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å –ø–æ—Ä–æ–≥–æ–º –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        logger.info(f"   –ò—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä–æ–≥: {original_threshold:.3f}")
        logger.info(f"   –ù–æ–≤—ã–π –ø–æ—Ä–æ–≥: {new_threshold:.3f}")
        logger.info(f"   –ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä: {len(json.load(open(Path(existing_results_dir) / 'predicted_pairs.json')))}")
        logger.info(f"   –ù–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä: {len(pairs)}")
        logger.info(f"   –†–∞–∑–Ω–∏—Ü–∞: {len(pairs) - len(json.load(open(Path(existing_results_dir) / 'predicted_pairs.json')))}")
        logger.info(f"   –ü–∞–ø–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: {output_dir}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        return {
            'pred_matrix': pred_matrix,
            'pairs': pairs,
            'terms': terms,
            'threshold': new_threshold,
            'original_threshold': original_threshold,
            'metadata': updated_metadata,
            'statistics': {
                'total_terms': len(terms),
                'total_pairs': len(pairs),
                'original_pairs': len(json.load(open(Path(existing_results_dir) / 'predicted_pairs.json'))),
                'pairs_difference': len(pairs) - len(json.load(open(Path(existing_results_dir) / 'predicted_pairs.json'))),
                'matrix_shape': pred_matrix.shape,
                'matrix_min': float(pred_matrix.min()),
                'matrix_max': float(pred_matrix.max()),
                'matrix_mean': float(pred_matrix.mean()),
                'matrix_std': float(pred_matrix.std())
            },
            'output_dir': output_dir,
            'files_created': {
                'pairs': str(pairs_file),
                'terms': str(terms_file),
                'matrix': str(matrix_file),
                'metadata': str(metadata_file),
                'distribution_plot': str(output_path / 'prediction_distribution.png')
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —Å –ø–æ—Ä–æ–≥–æ–º: {e}")
        raise


def run_inference(
    results_dir: str,
    terms_file: str,
    output_dir: str,
    embedding_batch_size: int = 32,
    prediction_batch_size: int = 64,
    custom_threshold: float = None,
    log_file: str = None,
    # –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –≥–æ—Ç–æ–≤–æ–π –º–∞—Ç—Ä–∏—Ü–µ–π
    existing_results_dir: str = None,
    threshold_only: bool = False
) -> Dict:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Jupyter notebook
    
    Args:
        results_dir: Directory with training results (only needed if existing_results_dir is None)
        terms_file: Path to .txt file with terms (only needed if existing_results_dir is None)
        output_dir: Directory to save inference results
        embedding_batch_size: Batch size for embedding generation
        prediction_batch_size: Batch size for prediction matrix construction
        custom_threshold: Custom threshold (if None, uses best F1 threshold from training)
        log_file: Path to log file (optional)
        existing_results_dir: Directory with existing inference results (optional)
        threshold_only: Whether to return only the threshold and statistics (optional)
        
    Returns:
        results: Dictionary with inference results and statistics
    """
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger = setup_logging(log_file)
    
    # –†–µ–∂–∏–º 1: –†–∞–±–æ—Ç–∞ —Å –≥–æ—Ç–æ–≤–æ–π –º–∞—Ç—Ä–∏—Ü–µ–π (—Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–π –ø–æ—Ä–æ–≥)
    if existing_results_dir is not None:
        logger.info("üéØ –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Å –≥–æ—Ç–æ–≤–æ–π –º–∞—Ç—Ä–∏—Ü–µ–π")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Ä–æ–≥–∞ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        if custom_threshold is None:
            logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Ä–æ–≥–∞...")
            _, _, f1_threshold = load_trained_model(results_dir)
            threshold = f1_threshold
        else:
            threshold = custom_threshold
        
        # –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —Å –ø–æ—Ä–æ–≥–æ–º
        return run_threshold_experiment(
            existing_results_dir=existing_results_dir,
            new_threshold=threshold,
            output_dir=output_dir,
            log_file=log_file
        )
    
    # –†–µ–∂–∏–º 2: –ü–æ–ª–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
    logger.info("üöÄ –†–µ–∂–∏–º –ø–æ–ª–Ω–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ Cross-Attention –º–æ–¥–µ–ª–∏")
    logger.info(f"üìÅ –ü–∞–ø–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: {results_dir}")
    logger.info(f"üìù –§–∞–π–ª —Å —Ç–µ—Ä–º–∏–Ω–∞–º–∏: {terms_file}")
    logger.info(f"üíæ –ü–∞–ø–∫–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞: {output_dir}")
    
    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
        model, best_results, f1_threshold = load_trained_model(results_dir)
        
        # –í—ã–±–æ—Ä –ø–æ—Ä–æ–≥–∞
        threshold = custom_threshold if custom_threshold is not None else f1_threshold
        
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞:")
        logger.info(f"   ROC AUC: {best_results['best_results']['roc_auc']:.4f}")
        logger.info(f"   –õ—É—á—à–∏–π F1 –ø–æ—Ä–æ–≥: {f1_threshold:.3f}")
        logger.info(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –ø–æ—Ä–æ–≥: {threshold:.3f}")
        
        # 2. –ß—Ç–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤
        logger.info("üìñ –ß—Ç–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤...")
        with open(terms_file, 'r', encoding='utf-8') as f:
            terms = [line.strip() for line in f if line.strip()]
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(terms)} —Ç–µ—Ä–º–∏–Ω–æ–≤")
        
        # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        embedding_model, tokenizer = load_embedding_model()
        
        logger.info("‚ö° –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        embeddings = get_term_embeddings(terms, embedding_model, tokenizer, embedding_batch_size)
        
        logger.info(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã: {len(embeddings)} x {len(embeddings[0])}")
        
        # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –æ—Ç –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        del embedding_model, tokenizer
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        # 4. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        logger.info("üîÆ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
        pred_matrix = build_prediction_matrix(embeddings, model, prediction_batch_size, logger)
        
        logger.info(f"‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞: {pred_matrix.shape}")
        logger.info(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: min={pred_matrix.min():.4f}, max={pred_matrix.max():.4f}, mean={pred_matrix.mean():.4f}")
        
        # 5. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä
        logger.info("üéØ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä...")
        pairs = apply_threshold_and_extract_pairs(pred_matrix, terms, threshold, logger)
        
        # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        save_results(pred_matrix, pairs, terms, output_dir, threshold, best_results, logger)
        
        logger.info("‚úÖ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        logger.info(f"   –í—Å–µ–≥–æ —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(terms)}")
        logger.info(f"   –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä: {len(pairs)}")
        logger.info(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥: {threshold:.3f}")
        logger.info(f"   –ü–∞–ø–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: {output_dir}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        return {
            'pred_matrix': pred_matrix,
            'pairs': pairs,
            'terms': terms,
            'threshold': threshold,
            'best_results': best_results,
            'model_info': {
                'roc_auc': best_results['best_results']['roc_auc'],
                'f1_threshold': f1_threshold,
                'dataset': best_results['experiment_info']['dataset']
            },
            'statistics': {
                'total_terms': len(terms),
                'total_pairs': len(pairs),
                'matrix_shape': pred_matrix.shape,
                'matrix_min': float(pred_matrix.min()),
                'matrix_max': float(pred_matrix.max()),
                'matrix_mean': float(pred_matrix.mean()),
                'matrix_std': float(pred_matrix.std())
            },
            'output_dir': output_dir
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {e}")
        raise


def main():
    parser = argparse.ArgumentParser(description="Cross-Attention Model Inference")
    parser.add_argument("--results_dir", type=str, required=True,
                        help="Directory with training results (e.g., results/20250707_192128_MatOnto_...)")
    parser.add_argument("--terms_file", type=str, required=True,
                        help="Path to .txt file with terms")
    parser.add_argument("--output_dir", type=str, required=True,
                        help="Directory to save inference results")
    parser.add_argument("--embedding_batch_size", type=int, default=32,
                        help="Batch size for embedding generation")
    parser.add_argument("--prediction_batch_size", type=int, default=64,
                        help="Batch size for prediction matrix construction")
    parser.add_argument("--custom_threshold", type=float, default=None,
                        help="Custom threshold (if not provided, uses best F1 threshold from training)")
    parser.add_argument("--log_file", type=str, default=None,
                        help="Path to log file")
    parser.add_argument("--existing_results_dir", type=str, default=None,
                        help="Directory with existing inference results (optional)")
    parser.add_argument("--threshold_only", action="store_true",
                        help="Whether to return only the threshold and statistics")
    
    args = parser.parse_args()
    
    # –í—ã–∑–æ–≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
    results = run_inference(
        results_dir=args.results_dir,
        terms_file=args.terms_file,
        output_dir=args.output_dir,
        embedding_batch_size=args.embedding_batch_size,
        prediction_batch_size=args.prediction_batch_size,
        custom_threshold=args.custom_threshold,
        log_file=args.log_file,
        existing_results_dir=args.existing_results_dir,
        threshold_only=args.threshold_only
    )
    
    # –í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π —Å–≤–æ–¥–∫–∏
    print(f"\nüìà –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞:")
    print(f"   –¢–µ—Ä–º–∏–Ω–æ–≤: {results['statistics']['total_terms']}")
    print(f"   –ü–∞—Ä: {results['statistics']['total_pairs']}")
    print(f"   –ü–æ—Ä–æ–≥: {results['threshold']:.3f}")
    print(f"   ROC AUC –º–æ–¥–µ–ª–∏: {results['model_info']['roc_auc']:.4f}")
    print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {results['output_dir']}")


if __name__ == "__main__":
    main() 