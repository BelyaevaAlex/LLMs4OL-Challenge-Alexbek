#!/usr/bin/env python3
"""
–ò–Ω—Ñ–µ—Ä–µ–Ω—Å –¥–ª—è Meta-Model
–õ–∞–∫–æ–Ω–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–π –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏–∑ —Å–ø–∏—Å–∫–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤
"""

import json
import numpy as np
import torch
from typing import Dict, List, Optional
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')
from tqdm import tqdm

from meta_model import Qwen3CrossAttentionMetaModel


def run_meta_model_inference(
    terms: List[str],
    model: Qwen3CrossAttentionMetaModel,
    batch_size: int = 32,
    device: str = "auto",
    save_path: str = None,
) -> Dict:
    """
    –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ Meta-Model –Ω–∞ —Å–ø–∏—Å–∫–µ —Ç–µ—Ä–º–∏–Ω–æ–≤
    
    Args:
        terms: —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        model: –æ–±—É—á–µ–Ω–Ω–∞—è Meta-Model
        batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        
    Returns:
        results: {
            'prediction_matrix': np.ndarray,  # –ü–æ–ª–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π terms x terms
            'terms': List[str],               # –ò—Å—Ö–æ–¥–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
            'statistics': Dict                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        }
    """
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    n_terms = len(terms)
    pred_matrix = np.zeros((n_terms, n_terms), dtype=np.float32)
    
    model.eval()
    
    print(f"üöÄ –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {n_terms} —Ç–µ—Ä–º–∏–Ω–æ–≤, batch_size={batch_size}")
    
    with torch.no_grad():
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ –±–ª–æ–∫–∞–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        for i in tqdm(range(0, n_terms, batch_size), desc="Inference"):
            end_i = min(i + batch_size, n_terms)
            batch_terms_1 = terms[i:end_i]
            
            for j in range(0, n_terms, batch_size):
                end_j = min(j + batch_size, n_terms)
                batch_terms_2 = terms[j:end_j]
                
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –±–ª–æ–∫–∞
                batch_pred = model(batch_terms_1, batch_terms_2)
                pred_matrix[i:end_i, j:end_j] = batch_pred.cpu().numpy()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    statistics = {
        'num_terms': n_terms,
        'matrix_shape': pred_matrix.shape,
        'matrix_min': float(pred_matrix.min()),
        'matrix_max': float(pred_matrix.max()),
        'matrix_mean': float(pred_matrix.mean()),
        'matrix_std': float(pred_matrix.std())
    }
    
    print(f"‚úÖ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω:")
    print(f"   –ú–∞—Ç—Ä–∏—Ü–∞: {pred_matrix.shape}")
    print(f"   –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: min={statistics['matrix_min']:.4f}, max={statistics['matrix_max']:.4f}, mean={statistics['matrix_mean']:.4f}")
    
    results = {
        'prediction_matrix': pred_matrix,
        'terms': terms,
        'statistics': statistics
    }
    
    if save_path:
        save_results(results, save_path)
    
    return results


def threshold_analysis(
    pred_matrix: np.ndarray,
    gt_relationships: List[Dict],
    terms: List[str],
    thresholds: Optional[List[float]] = None
) -> Dict:
    """
    –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤ –¥–ª—è –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    
    Args:
        pred_matrix: –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        gt_relationships: —Å–ø–∏—Å–æ–∫ –æ—Ç–Ω–æ—à–µ–Ω–∏–π {"parent": "...", "child": "..."}
        terms: —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤
        thresholds: –ø–æ—Ä–æ–≥–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
    Returns:
        analysis: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    """
    if thresholds is None:
        thresholds = np.arange(0.01, 0.95, 0.01)
    
    # –°–æ–∑–¥–∞–µ–º ground truth –º–∞—Ç—Ä–∏—Ü—É
    term_to_idx = {term: i for i, term in enumerate(terms)}
    gt_matrix = np.zeros_like(pred_matrix)
    
    for rel in gt_relationships:
        parent = rel.get('parent')
        child = rel.get('child')
        if parent in term_to_idx and child in term_to_idx:
            child_idx = term_to_idx[child]
            parent_idx = term_to_idx[parent]
            gt_matrix[child_idx, parent_idx] = 1
    
    # –£–±–∏—Ä–∞–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    mask = np.eye(pred_matrix.shape[0], dtype=bool)
    pred_flat = pred_matrix[~mask]
    gt_flat = gt_matrix[~mask]
    
    results = {
        'thresholds': thresholds.tolist(),
        'accuracy': [],
        'precision': [],
        'recall': [],
        'f1': []
    }
    
    for threshold in thresholds:
        pred_binary = (pred_flat > threshold).astype(int)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        tp = np.sum((pred_binary == 1) & (gt_flat == 1))
        fp = np.sum((pred_binary == 1) & (gt_flat == 0))
        tn = np.sum((pred_binary == 0) & (gt_flat == 0))
        fn = np.sum((pred_binary == 0) & (gt_flat == 1))
        
        acc = (tp + tn) / (tp + fp + tn + fn) if (tp + fp + tn + fn) > 0 else 0
        prec = tp / (tp + fp) if (tp + fp) > 0 else 0
        rec = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0
        
        results['accuracy'].append(acc)
        results['precision'].append(prec)
        results['recall'].append(rec)
        results['f1'].append(f1)
    
    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–µ –ø–æ—Ä–æ–≥–∏
    best_thresholds = {
        'accuracy': thresholds[np.argmax(results['accuracy'])],
        'precision': thresholds[np.argmax(results['precision'])],
        'recall': thresholds[np.argmax(results['recall'])],
        'f1': thresholds[np.argmax(results['f1'])]
    }
    
    best_values = {
        'accuracy': np.max(results['accuracy']),
        'precision': np.max(results['precision']),
        'recall': np.max(results['recall']),
        'f1': np.max(results['f1'])
    }
    
    return {
        'threshold_analysis': results,
        'best_thresholds': best_thresholds,
        'best_values': best_values,
        'gt_stats': {
            'total_relationships': len(gt_relationships),
            'positive_pairs': int(gt_flat.sum()),
            'negative_pairs': int((gt_flat == 0).sum()),
            'positive_ratio': float(gt_flat.sum() / len(gt_flat))
        }
    }


def plot_prediction_distribution(
    pred_matrix: np.ndarray,
    thresholds: List[float] = None,
    save_path: str = None
) -> str:
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ –¥–æ–ª–∏ –æ—Ç–≤–µ—Ç–æ–≤ –±–æ–ª—å—à–µ –ø–æ—Ä–æ–≥–æ–≤
    
    Args:
        pred_matrix: –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        thresholds: –ø–æ—Ä–æ–≥–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        
    Returns:
        plot_path: –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É –≥—Ä–∞—Ñ–∏–∫—É
    """
    if thresholds is None:
        thresholds = [0.05, 0.15, 0.25, 0.5, 0.75]
    
    # –£–±–∏—Ä–∞–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    mask = np.eye(pred_matrix.shape[0], dtype=bool)
    off_diagonal = pred_matrix[~mask]
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    ax1.hist(off_diagonal, bins=50, alpha=0.7, edgecolor='black')
    for thr in thresholds:
        ax1.axvline(thr, color='red', linestyle='--', alpha=0.7, label=f'{thr:.2f}')
    ax1.set_xlabel('Prediction Score')
    ax1.set_ylabel('Frequency')
    ax1.set_title('Distribution of Prediction Scores')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞
    ax2.hist(off_diagonal, bins=50, alpha=0.7, edgecolor='black')
    for thr in thresholds:
        ax2.axvline(thr, color='red', linestyle='--', alpha=0.7)
    ax2.set_xlabel('Prediction Score')
    ax2.set_ylabel('Frequency')
    ax2.set_title('Distribution (Log Scale)')
    ax2.set_yscale('log')
    ax2.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 3: –î–æ–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –±–æ–ª—å—à–µ –ø–æ—Ä–æ–≥–æ–≤
    threshold_range = np.arange(0.01, 0.95, 0.01)
    ratios = []
    for thr in threshold_range:
        ratio = np.sum(off_diagonal > thr) / len(off_diagonal)
        ratios.append(ratio)
    
    ax3.plot(threshold_range, ratios, 'b-', linewidth=2)
    for thr in thresholds:
        ratio = np.sum(off_diagonal > thr) / len(off_diagonal)
        ax3.axvline(thr, color='red', linestyle='--', alpha=0.7)
        ax3.text(thr, ratio + 0.02, f'{ratio:.3f}', ha='center', va='bottom')
    ax3.set_xlabel('Threshold')
    ax3.set_ylabel('Ratio of Predictions > Threshold')
    ax3.set_title('Ratio of Predictions Above Threshold')
    ax3.grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 4: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats_text = f"""
Matrix Statistics:

Shape: {pred_matrix.shape}
Total pairs: {pred_matrix.shape[0] * (pred_matrix.shape[1] - 1)}

Score Distribution:
Min: {off_diagonal.min():.4f}
Max: {off_diagonal.max():.4f}
Mean: {off_diagonal.mean():.4f}
Std: {off_diagonal.std():.4f}

Threshold Analysis:
"""
    for thr in thresholds:
        ratio = np.sum(off_diagonal > thr) / len(off_diagonal)
        stats_text += f">{thr:.2f}: {ratio:.3f} ({np.sum(off_diagonal > thr):,} pairs)\n"
    
    ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, 
             verticalalignment='top', fontsize=9, fontfamily='monospace')
    ax4.axis('off')
    
    plt.tight_layout()
    
    if save_path:
        plot_path = save_path
    else:
        plot_path = 'prediction_distribution.png'
    
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {plot_path}")
    return plot_path


def extract_relationships_from_matrix(
    pred_matrix: np.ndarray,
    terms: List[str],
    threshold: float,
    remove_self_loops: bool = True,
    min_confidence: float = None,
    max_relationships: int = None
) -> List[Dict[str, str]]:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ relationships –∏–∑ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ TaskC
    
    Args:
        pred_matrix: –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Ä–∞–∑–º–µ—Ä–∞ (n_terms, n_terms)
        terms: —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Å—Ç—Ä–æ–∫–∞–º/—Å—Ç–æ–ª–±—Ü–∞–º –º–∞—Ç—Ä–∏—Ü—ã
        threshold: –ø–æ—Ä–æ–≥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ—Ç–Ω–æ—à–µ–Ω–∏–π
        remove_self_loops: —É–±—Ä–∞—Ç—å –¥–∏–∞–≥–æ–Ω–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (self-loops)
        min_confidence: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è)
        max_relationships: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–Ω–æ—à–µ–Ω–∏–π (—Ç–æ–ø –ø–æ confidence)
        
    Returns:
        relationships: —Å–ø–∏—Å–æ–∫ –æ—Ç–Ω–æ—à–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ [{"ID": "...", "parent": "...", "child": "..."}, ...]
        
    Note:
        pred_matrix[i][j] > threshold –æ–∑–Ω–∞—á–∞–µ—Ç —á—Ç–æ terms[i] —è–≤–ª—è–µ—Ç—Å—è child —Ç–µ—Ä–º–∏–Ω–∞ terms[j] (parent)
    """
    import uuid
    
    n_terms = len(terms)
    relationships = []
    
    print(f"üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ relationships: threshold={threshold:.4f}")
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —Å –∏—Ö confidence
    candidates = []
    
    for i in range(n_terms):
        for j in range(n_terms):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if remove_self_loops and i == j:
                continue
                
            confidence = float(pred_matrix[i, j])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º threshold
            if confidence > threshold:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ min_confidence
                if min_confidence is not None and confidence < min_confidence:
                    continue
                    
                candidates.append({
                    'child_idx': i,
                    'parent_idx': j,
                    'confidence': confidence,
                    'child': terms[i],
                    'parent': terms[j]
                })
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é confidence
    candidates.sort(key=lambda x: x['confidence'], reverse=True)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if max_relationships is not None:
        candidates = candidates[:max_relationships]
    
    # –°–æ–∑–¥–∞–µ–º relationships –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    for idx, candidate in enumerate(candidates):
        relationship_id = f"REL_{uuid.uuid4().hex[:8]}"
        
        relationship = {
            "ID": relationship_id,
            "parent": candidate['parent'],
            "child": candidate['child']
        }
        
        relationships.append(relationship)
    
    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(relationships)} relationships")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if len(relationships) > 0:
        print(f"   –î–∏–∞–ø–∞–∑–æ–Ω confidence: {candidates[0]['confidence']:.4f} - {candidates[-1]['confidence']:.4f}")
        confidences = [c['confidence'] for c in candidates]
        print(f"   –°—Ä–µ–¥–Ω—è—è confidence: {np.mean(confidences):.4f} ¬± {np.std(confidences):.4f}")
        
        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        unique_parents = set(r['parent'] for r in relationships)
        unique_children = set(r['child'] for r in relationships)
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ parents: {len(unique_parents)}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ children: {len(unique_children)}")
        
        # –¢–æ–ø-3 relationships
        print(f"   –¢–æ–ø-3 relationships:")
        for i, rel in enumerate(relationships[:3]):
            conf = candidates[i]['confidence']
            print(f"     {i+1}. {rel['child']} -> {rel['parent']} (conf: {conf:.4f})")
    
    return relationships


def save_relationships_to_json(
    relationships: List[Dict[str, str]],
    save_path: str,
    metadata: Dict = None
):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ relationships –≤ JSON —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ TaskC
    
    Args:
        relationships: —Å–ø–∏—Å–æ–∫ relationships
        save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        metadata: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª
    """
    save_path = Path(save_path)
    
    # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –ø–∞–ø–∫–∞, —Å–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞
    if save_path.is_dir() or not save_path.suffix:
        save_path = save_path / "predicted_relationships.json"
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    save_path.parent.mkdir(parents=True, exist_ok=True)
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    output_data = relationships
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –≤ –Ω–∞—á–∞–ª–æ –µ—Å–ª–∏ –µ—Å—Ç—å
    if metadata:
        # JSON –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, –Ω–æ –º—ã –º–æ–∂–µ–º –¥–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª
        metadata_path = save_path.with_suffix('.metadata.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        print(f"üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º relationships
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ Relationships —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {save_path}")
    print(f"   –í—Å–µ–≥–æ –æ—Ç–Ω–æ—à–µ–Ω–∏–π: {len(relationships)}")
    
    return str(save_path)


def save_results(
    results: Dict,
    save_path: str,
    threshold_analysis: Dict = None,
    relationships: List[Dict] = None
):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞"""
    save_dir = Path(save_path)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã
    np.save(save_dir / "prediction_matrix.npy", results['prediction_matrix'])
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤
    with open(save_dir / "terms.json", 'w', encoding='utf-8') as f:
        json.dump(results['terms'], f, ensure_ascii=False, indent=2)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    metadata = {
        'statistics': results['statistics'],
        'threshold_analysis': threshold_analysis,
        'relationships': relationships
    }
    
    with open(save_dir / "results.json", 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {save_path}")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    print("üöÄ –õ–∞–∫–æ–Ω–∏—á–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å Meta-Model")
    print("–ü—Ä–∏–º–µ—Ä:")
    print("""
    from dataset import MetaModelDataset
    from meta_model import Qwen3CrossAttentionMetaModel
    from inference_meta_model import (
        run_meta_model_inference, 
        threshold_analysis, 
        plot_prediction_distribution,
        extract_relationships_from_matrix,
        save_relationships_to_json
    )
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    dataset = MetaModelDataset(terms_path="data/terms.txt", relations_path="data/relations.json")
    terms = dataset.get_all_terms()
    gt_relationships = dataset.get_all_relations()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
    model = Qwen3CrossAttentionMetaModel.from_pretrained("path/to/best_model")
    results = run_meta_model_inference(terms, model, batch_size=32)
    pred_matrix = results['prediction_matrix']
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ—Ä–æ–≥–æ–≤
    analysis = threshold_analysis(pred_matrix, gt_relationships, terms)
    best_threshold = analysis['best_thresholds']['f1']
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ relationships —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
    relationships = extract_relationships_from_matrix(
        pred_matrix=pred_matrix,
        terms=terms,
        threshold=best_threshold,
        remove_self_loops=True,
        max_relationships=10000  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ relationships –≤ —Ñ–æ—Ä–º–∞—Ç–µ TaskC
    metadata = {
        'threshold': best_threshold,
        'num_terms': len(terms),
        'model_info': model.get_model_info(),
        'statistics': results['statistics'],
        'threshold_analysis': analysis
    }
    
    save_relationships_to_json(
        relationships=relationships,
        save_path="predicted_relationships.json",
        metadata=metadata
    )
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    plot_prediction_distribution(pred_matrix, save_path="distribution.png")
    save_results(results, "inference_results/", analysis, relationships)
    
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(relationships)} relationships —Å –ø–æ—Ä–æ–≥–æ–º {best_threshold:.4f}")
    """) 