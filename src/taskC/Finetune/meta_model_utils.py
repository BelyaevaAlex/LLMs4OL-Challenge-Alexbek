"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Qwen3CrossAttentionMetaModel

–í–∫–ª—é—á–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è:
- –°–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
- –û—Ü–µ–Ω–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
from typing import List, Dict, Tuple, Optional, Any
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
import pandas as pd
import time

from meta_model import Qwen3CrossAttentionMetaModel


class MetaModelDataset(Dataset):
    """
    –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–µ—Ç–∞-–º–æ–¥–µ–ª—å—é
    
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
    [
        {
            "phrases_1": ["phrase1", "phrase2"],
            "phrases_2": ["phrase3", "phrase4"],
            "labels": [[0, 1], [1, 0]]  # –º–∞—Ç—Ä–∏—Ü–∞ –æ—Ç–Ω–æ—à–µ–Ω–∏–π
        }
    ]
    """
    
    def __init__(self, examples: List[Dict[str, Any]]):
        self.examples = examples
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        example = self.examples[idx]
        return {
            "phrases_1": example["phrases_1"],
            "phrases_2": example["phrases_2"],
            "labels": torch.tensor(example["labels"], dtype=torch.float32)
        }


def create_meta_dataset_from_cross_attention_data(
    entities_path: str,
    relations_path: str,
    sample_size: int = 1000,
    max_phrases_per_sample: int = 20,
    random_state: int = 42
) -> List[Dict[str, Any]]:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö CrossAttentionDataset
    
    Args:
        entities_path: –ø—É—Ç—å –∫ entities.json
        relations_path: –ø—É—Ç—å –∫ relations.json
        sample_size: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
        max_phrases_per_sample: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–∑ –≤ –æ–¥–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ
        random_state: seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        
    Returns:
        examples: —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è MetaModelDataset
    """
    import random
    random.seed(random_state)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    with open(entities_path, 'r', encoding='utf-8') as f:
        entities = json.load(f)
    
    with open(relations_path, 'r', encoding='utf-8') as f:
        relations = json.load(f)
    
    # –°–æ–∑–¥–∞–µ–º mapping –æ—Ç text_description –∫ ID
    text_to_id = {entity['text_description']: entity['id'] for entity in entities}
    id_to_text = {entity['id']: entity['text_description'] for entity in entities}
    
    # –°–æ–∑–¥–∞–µ–º set –æ—Ç–Ω–æ—à–µ–Ω–∏–π –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
    relation_set = set()
    for rel in relations:
        if isinstance(rel, dict):
            parent_text = rel.get('parent', '')
            child_text = rel.get('child', '')
            if parent_text and child_text:
                relation_set.add((parent_text, child_text))
        else:
            # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç [id1, id2]
            parent_id, child_id = rel
            parent_text = id_to_text.get(parent_id, '')
            child_text = id_to_text.get(child_id, '')
            if parent_text and child_text:
                relation_set.add((parent_text, child_text))
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
    all_terms = list(set(entity['text_description'] for entity in entities 
                        if entity.get('text_description', '')))
    
    examples = []
    
    for _ in range(sample_size):
        # –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º —Ñ—Ä–∞–∑—ã
        num_phrases = random.randint(5, max_phrases_per_sample)
        selected_terms = random.sample(all_terms, min(num_phrases, len(all_terms)))
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –æ—Ç–Ω–æ—à–µ–Ω–∏–π
        labels = []
        for child_term in selected_terms:
            row = []
            for parent_term in selected_terms:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ child -> parent
                has_relation = (child_term, parent_term) in relation_set
                row.append(1 if has_relation else 0)
            labels.append(row)
        
        examples.append({
            "phrases_1": selected_terms,
            "phrases_2": selected_terms,
            "labels": labels
        })
    
    return examples


def evaluate_meta_model(
    meta_model: Qwen3CrossAttentionMetaModel,
    test_examples: List[Dict[str, Any]],
    thresholds: List[float] = None,
    device: str = "auto"
) -> Dict[str, Any]:
    """
    –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
    
    Args:
        meta_model: –º–µ—Ç–∞-–º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        test_examples: —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        thresholds: –ø–æ—Ä–æ–≥–∏ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        
    Returns:
        results: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏
    """
    if thresholds is None:
        thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
    
    meta_model.eval()
    
    all_predictions = []
    all_labels = []
    
    print(f"–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ {len(test_examples)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")
    
    with torch.no_grad():
        for i, example in enumerate(test_examples):
            if i % 10 == 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(test_examples)} –ø—Ä–∏–º–µ—Ä–æ–≤")
            
            phrases_1 = example["phrases_1"]
            phrases_2 = example["phrases_2"]
            labels = example["labels"]
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            predictions = meta_model(phrases_1, phrases_2)
            
            # –ü—Ä–∏–≤–æ–¥–∏–º –∫ numpy
            predictions_np = predictions.cpu().numpy().flatten()
            labels_np = labels.numpy().flatten()
            
            all_predictions.extend(predictions_np)
            all_labels.extend(labels_np)
    
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    
    # –í—ã—á–∏—Å–ª—è–µ–º ROC-AUC
    try:
        roc_auc = roc_auc_score(all_labels, all_predictions)
    except:
        roc_auc = 0.0
    
    # –û—Ü–µ–Ω–∫–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤
    threshold_results = {}
    
    for threshold in thresholds:
        binary_predictions = (all_predictions > threshold).astype(int)
        
        try:
            accuracy = accuracy_score(all_labels, binary_predictions)
            precision = precision_score(all_labels, binary_predictions, zero_division=0)
            recall = recall_score(all_labels, binary_predictions, zero_division=0)
            f1 = f1_score(all_labels, binary_predictions, zero_division=0)
        except:
            accuracy = precision = recall = f1 = 0.0
        
        threshold_results[threshold] = {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1_score": f1
        }
    
    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π –ø–æ—Ä–æ–≥
    best_threshold = max(thresholds, key=lambda t: threshold_results[t]["f1_score"])
    
    results = {
        "roc_auc": roc_auc,
        "best_threshold": best_threshold,
        "best_metrics": threshold_results[best_threshold],
        "threshold_results": threshold_results,
        "num_examples": len(test_examples),
        "num_predictions": len(all_predictions),
        "positive_ratio": np.mean(all_labels)
    }
    
    return results


def visualize_evaluation_results(
    results: Dict[str, Any],
    save_path: str = None,
    show_plot: bool = True
):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏
    
    Args:
        results: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –æ—Ç evaluate_meta_model
        save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
        show_plot: –ø–æ–∫–∞–∑–∞—Ç—å –ª–∏ –≥—Ä–∞—Ñ–∏–∫
    """
    threshold_results = results["threshold_results"]
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
    thresholds = list(threshold_results.keys())
    accuracies = [threshold_results[t]["accuracy"] for t in thresholds]
    precisions = [threshold_results[t]["precision"] for t in thresholds]
    recalls = [threshold_results[t]["recall"] for t in thresholds]
    f1_scores = [threshold_results[t]["f1_score"] for t in thresholds]
    
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    plt.plot(thresholds, accuracies, 'b-', marker='o', label='Accuracy')
    plt.axvline(x=results["best_threshold"], color='r', linestyle='--', alpha=0.7, label=f'Best threshold: {results["best_threshold"]}')
    plt.xlabel('Threshold')
    plt.ylabel('Accuracy')
    plt.title('Accuracy vs Threshold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 2, 2)
    plt.plot(thresholds, precisions, 'g-', marker='o', label='Precision')
    plt.axvline(x=results["best_threshold"], color='r', linestyle='--', alpha=0.7, label=f'Best threshold: {results["best_threshold"]}')
    plt.xlabel('Threshold')
    plt.ylabel('Precision')
    plt.title('Precision vs Threshold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 2, 3)
    plt.plot(thresholds, recalls, 'orange', marker='o', label='Recall')
    plt.axvline(x=results["best_threshold"], color='r', linestyle='--', alpha=0.7, label=f'Best threshold: {results["best_threshold"]}')
    plt.xlabel('Threshold')
    plt.ylabel('Recall')
    plt.title('Recall vs Threshold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 2, 4)
    plt.plot(thresholds, f1_scores, 'purple', marker='o', label='F1 Score')
    plt.axvline(x=results["best_threshold"], color='r', linestyle='--', alpha=0.7, label=f'Best threshold: {results["best_threshold"]}')
    plt.xlabel('Threshold')
    plt.ylabel('F1 Score')
    plt.title('F1 Score vs Threshold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
    plt.suptitle(f'Meta-Model Evaluation Results (ROC-AUC: {results["roc_auc"]:.4f})', 
                 fontsize=16, y=1.02)
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"–ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}")
    
    if show_plot:
        plt.show()


def benchmark_meta_model_performance(
    meta_model: Qwen3CrossAttentionMetaModel,
    phrase_counts: List[int] = None,
    num_trials: int = 5
) -> Dict[str, Any]:
    """
    –ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
    
    Args:
        meta_model: –º–µ—Ç–∞-–º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        phrase_counts: –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ—Ä–∞–∑ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        num_trials: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø—ã—Ç–∞–Ω–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
        
    Returns:
        benchmark_results: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞
    """
    if phrase_counts is None:
        phrase_counts = [5, 10, 20, 50, 100]
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ—Ä–∞–∑—ã
    test_phrases = [
        f"test phrase {i}" for i in range(max(phrase_counts))
    ]
    
    results = {}
    
    meta_model.eval()
    
    for phrase_count in phrase_counts:
        print(f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å {phrase_count} —Ñ—Ä–∞–∑–∞–º–∏...")
        
        current_phrases = test_phrases[:phrase_count]
        times = []
        
        for trial in range(num_trials):
            start_time = time.time()
            
            with torch.no_grad():
                _ = meta_model(current_phrases, current_phrases)
            
            end_time = time.time()
            times.append(end_time - start_time)
        
        avg_time = np.mean(times)
        std_time = np.std(times)
        
        results[phrase_count] = {
            "avg_time": avg_time,
            "std_time": std_time,
            "times": times,
            "matrix_size": (phrase_count, phrase_count),
            "total_comparisons": phrase_count * phrase_count
        }
        
        print(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.4f} ¬± {std_time:.4f} —Å–µ–∫")
        print(f"  –í—Ä–µ–º—è –Ω–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ: {avg_time / (phrase_count * phrase_count) * 1000:.2f} –º—Å")
    
    return results


def visualize_performance_benchmark(
    benchmark_results: Dict[str, Any],
    save_path: str = None,
    show_plot: bool = True
):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∞
    
    Args:
        benchmark_results: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç benchmark_meta_model_performance
        save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
        show_plot: –ø–æ–∫–∞–∑–∞—Ç—å –ª–∏ –≥—Ä–∞—Ñ–∏–∫
    """
    phrase_counts = list(benchmark_results.keys())
    avg_times = [benchmark_results[count]["avg_time"] for count in phrase_counts]
    std_times = [benchmark_results[count]["std_time"] for count in phrase_counts]
    
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.errorbar(phrase_counts, avg_times, yerr=std_times, 
                marker='o', capsize=5, capthick=2, linewidth=2)
    plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–∑')
    plt.ylabel('–í—Ä–µ–º—è (—Å–µ–∫—É–Ω–¥—ã)')
    plt.title('–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ vs –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–∑')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    time_per_comparison = [avg_times[i] / (phrase_counts[i] ** 2) * 1000 
                          for i in range(len(phrase_counts))]
    plt.plot(phrase_counts, time_per_comparison, 'r-', marker='o', linewidth=2)
    plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–∑')
    plt.ylabel('–í—Ä–µ–º—è –Ω–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ (–º—Å)')
    plt.title('–í—Ä–µ–º—è –Ω–∞ –æ–¥–Ω–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"–ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}")
    
    if show_plot:
        plt.show()


def convert_cross_attention_results_to_meta_format(
    terms_file: str,
    prediction_matrix_file: str,
    threshold: float = 0.5
) -> List[Dict[str, str]]:
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ CrossAttention –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
    
    Args:
        terms_file: —Ñ–∞–π–ª —Å —Ç–µ—Ä–º–∏–Ω–∞–º–∏
        prediction_matrix_file: —Ñ–∞–π–ª —Å –º–∞—Ç—Ä–∏—Ü–µ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        threshold: –ø–æ—Ä–æ–≥ –¥–ª—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏
        
    Returns:
        relationships: —Å–ø–∏—Å–æ–∫ –æ—Ç–Ω–æ—à–µ–Ω–∏–π
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã
    with open(terms_file, 'r', encoding='utf-8') as f:
        terms = [line.strip() for line in f.readlines()]
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    prediction_matrix = np.load(prediction_matrix_file)
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥ –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –æ—Ç–Ω–æ—à–µ–Ω–∏—è
    binary_matrix = (prediction_matrix > threshold).astype(int)
    
    relationships = []
    for i in range(len(terms)):
        for j in range(len(terms)):
            if binary_matrix[i, j] and i != j:  # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∞–º–æ–∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è
                relationships.append({
                    "child": terms[i],
                    "parent": terms[j],
                    "confidence": float(prediction_matrix[i, j])
                })
    
    return relationships


def save_evaluation_report(
    results: Dict[str, Any],
    meta_model_info: Dict[str, Any],
    save_path: str
):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ–± –æ—Ü–µ–Ω–∫–µ –≤ JSON
    
    Args:
        results: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏
        meta_model_info: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏
        save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞
    """
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "meta_model_info": meta_model_info,
        "evaluation_results": results,
        "summary": {
            "best_threshold": results["best_threshold"],
            "best_f1_score": results["best_metrics"]["f1_score"],
            "best_accuracy": results["best_metrics"]["accuracy"],
            "best_precision": results["best_metrics"]["precision"],
            "best_recall": results["best_metrics"]["recall"],
            "roc_auc": results["roc_auc"]
        }
    }
    
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {save_path}")


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —É—Ç–∏–ª–∏—Ç
    print("üîß –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Ç–∏–ª–∏—Ç –¥–ª—è –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏...")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    test_examples = [
        {
            "phrases_1": ["machine learning", "deep learning"],
            "phrases_2": ["artificial intelligence", "computer science"],
            "labels": [[0.8, 0.9], [0.7, 0.8]]
        },
        {
            "phrases_1": ["neural networks", "algorithms"],
            "phrases_2": ["programming", "mathematics"],
            "labels": [[0.6, 0.7], [0.5, 0.9]]
        }
    ]
    
    print("‚úÖ –£—Ç–∏–ª–∏—Ç—ã –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!") 