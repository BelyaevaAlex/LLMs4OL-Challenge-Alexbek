"""
Analysis of Cross-Attention model inference results
Creating statistics, visualizations and exports in different formats
"""

import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')

from pathlib import Path
import argparse
from typing import Dict, List, Tuple
from collections import Counter, defaultdict
import networkx as nx
from datetime import datetime


def load_inference_results(results_dir: str) -> Tuple[np.ndarray, List[Dict], List[str], Dict]:
    """
    Load inference results
    
    Args:
        results_dir: directory with results
        
    Returns:
        pred_matrix: prediction matrix
        pairs: list of pairs
        terms: list of terms
        metadata: metadata
    """
    results_path = Path(results_dir)
    
    # Load matrix
    matrix_file = results_path / "prediction_matrix.npy"
    pred_matrix = np.load(matrix_file)
    
    # Load pairs
    pairs_file = results_path / "predicted_pairs.json"
    with open(pairs_file, 'r', encoding='utf-8') as f:
        pairs = json.load(f)
    
    # Load terms
    terms_file = results_path / "terms.json"
    with open(terms_file, 'r', encoding='utf-8') as f:
        terms = json.load(f)
    
    # Load metadata
    metadata_file = results_path / "inference_metadata.json"
    with open(metadata_file, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    return pred_matrix, pairs, terms, metadata


def analyze_hierarchy_structure(pairs: List[Dict], terms: List[str]) -> Dict:
    """
    –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–µ—Ä–∞—Ä—Ö–∏–∏
    
    Args:
        pairs: —Å–ø–∏—Å–æ–∫ –ø–∞—Ä child-parent
        terms: —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        
    Returns:
        analysis: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    """
    
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ
    G = nx.DiGraph()
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Ç–µ—Ä–º–∏–Ω—ã –∫–∞–∫ —É–∑–ª—ã
    G.add_nodes_from(terms)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ä—ë–±—Ä–∞ –∏–∑ –ø–∞—Ä
    for pair in pairs:
        G.add_edge(pair['child'], pair['parent'])
    
    # –ü–æ–¥—Å—á—ë—Ç —Å—Ç–µ–ø–µ–Ω–µ–π —É–∑–ª–æ–≤
    parent_counts = Counter()
    child_counts = Counter()
    
    for pair in pairs:
        parent_counts[pair['parent']] += 1
        child_counts[pair['child']] += 1
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —É–∑–ª–æ–≤
    roots = []  # –¢–æ–ª—å–∫–æ —Ä–æ–¥–∏—Ç–µ–ª–∏, –Ω–µ –¥–µ—Ç–∏
    leaves = []  # –¢–æ–ª—å–∫–æ –¥–µ—Ç–∏, –Ω–µ —Ä–æ–¥–∏—Ç–µ–ª–∏
    intermediate = []  # –ò —Ä–æ–¥–∏—Ç–µ–ª–∏, –∏ –¥–µ—Ç–∏
    isolated = []  # –ù–∏ —Ä–æ–¥–∏—Ç–µ–ª–∏, –Ω–∏ –¥–µ—Ç–∏
    
    for term in terms:
        is_parent = term in parent_counts
        is_child = term in child_counts
        
        if is_parent and not is_child:
            roots.append(term)
        elif is_child and not is_parent:
            leaves.append(term)
        elif is_parent and is_child:
            intermediate.append(term)
        else:
            isolated.append(term)
    
    # –ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–Ω–æ—Å—Ç–∏
    connected_components = list(nx.weakly_connected_components(G))
    largest_component = max(connected_components, key=len) if connected_components else set()
    
    # –ê–Ω–∞–ª–∏–∑ —Ü–∏–∫–ª–æ–≤
    cycles = list(nx.simple_cycles(G))
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞
    max_depth = 0
    for root in roots:
        try:
            depths = nx.single_source_shortest_path_length(G, root)
            max_depth = max(max_depth, max(depths.values()) if depths else 0)
        except:
            pass
    
    analysis = {
        'basic_stats': {
            'total_terms': len(terms),
            'total_pairs': len(pairs),
            'unique_parents': len(parent_counts),
            'unique_children': len(child_counts),
            'density': len(pairs) / (len(terms) * (len(terms) - 1)) if len(terms) > 1 else 0
        },
        'node_classification': {
            'roots': len(roots),
            'leaves': len(leaves),
            'intermediate': len(intermediate),
            'isolated': len(isolated),
            'root_terms': roots[:10],  # –ü–µ—Ä–≤—ã–µ 10 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
            'leaf_terms': leaves[:10],
            'top_parents': parent_counts.most_common(10),
            'top_children': child_counts.most_common(10)
        },
        'connectivity': {
            'connected_components': len(connected_components),
            'largest_component_size': len(largest_component),
            'largest_component_ratio': len(largest_component) / len(terms) if terms else 0,
            'average_component_size': np.mean([len(c) for c in connected_components]) if connected_components else 0
        },
        'structure': {
            'cycles_count': len(cycles),
            'max_depth': max_depth,
            'cycles': cycles[:5]  # –ü–µ—Ä–≤—ã–µ 5 —Ü–∏–∫–ª–æ–≤ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
        }
    }
    
    return analysis


def create_detailed_visualizations(
    pred_matrix: np.ndarray,
    pairs: List[Dict],
    terms: List[str],
    metadata: Dict,
    output_dir: str
):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
    
    Args:
        pred_matrix: –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        pairs: —Å–ø–∏—Å–æ–∫ –ø–∞—Ä
        terms: —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤
        metadata: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        output_dir: –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    output_path = Path(output_dir)
    
    # 1. –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ (–¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –º–∞—Ç—Ä–∏—Ü)
    if len(terms) <= 50:
        plt.figure(figsize=(12, 10))
        im = plt.imshow(pred_matrix, cmap='viridis', aspect='auto')
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–µ—Ç–æ–∫ –æ—Å–µ–π
        plt.xticks(range(len(terms)), terms, rotation=45, ha='right')
        plt.yticks(range(len(terms)), terms, rotation=0)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ colorbar
        cbar = plt.colorbar(im)
        cbar.set_label('Prediction Score')
        
        plt.title('Prediction Matrix Heatmap')
        plt.xlabel('Parent (columns)')
        plt.ylabel('Child (rows)')
        plt.tight_layout()
        plt.savefig(output_path / 'prediction_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    # 2. –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ —Ç–µ—Ä–º–∏–Ω–∞–º
    plt.figure(figsize=(15, 10))
    
    # –ü–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–≤—è–∑–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
    parent_counts = Counter()
    child_counts = Counter()
    
    for pair in pairs:
        parent_counts[pair['parent']] += 1
        child_counts[pair['child']] += 1
    
    # –¢–æ–ø —Ä–æ–¥–∏—Ç–µ–ª–µ–π
    plt.subplot(2, 2, 1)
    top_parents = parent_counts.most_common(20)
    if top_parents:
        parents, counts = zip(*top_parents)
        plt.barh(range(len(parents)), counts)
        plt.yticks(range(len(parents)), parents)
        plt.xlabel('Number of Children')
        plt.title('Top 20 Parents (by number of children)')
        plt.gca().invert_yaxis()
    
    # –¢–æ–ø –¥–µ—Ç–µ–π
    plt.subplot(2, 2, 2)
    top_children = child_counts.most_common(20)
    if top_children:
        children, counts = zip(*top_children)
        plt.barh(range(len(children)), counts)
        plt.yticks(range(len(children)), children)
        plt.xlabel('Number of Parents')
        plt.title('Top 20 Children (by number of parents)')
        plt.gca().invert_yaxis()
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–µ—Ç–µ–π
    plt.subplot(2, 2, 3)
    if parent_counts:
        plt.hist(list(parent_counts.values()), bins=20, alpha=0.7, edgecolor='black')
        plt.xlabel('Number of Children')
        plt.ylabel('Number of Parents')
        plt.title('Distribution of Children per Parent')
        plt.grid(True, alpha=0.3)
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–æ–¥–∏—Ç–µ–ª–µ–π
    plt.subplot(2, 2, 4)
    if child_counts:
        plt.hist(list(child_counts.values()), bins=20, alpha=0.7, edgecolor='black')
        plt.xlabel('Number of Parents')
        plt.ylabel('Number of Children')
        plt.title('Distribution of Parents per Child')
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_path / 'hierarchy_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. –ì—Ä–∞—Ñ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –≥—Ä–∞—Ñ–æ–≤)
    if len(terms) <= 30:
        plt.figure(figsize=(15, 12))
        
        G = nx.DiGraph()
        G.add_nodes_from(terms)
        
        for pair in pairs:
            G.add_edge(pair['child'], pair['parent'])
        
        # –ü–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —É–∑–ª–æ–≤
        pos = nx.spring_layout(G, k=3, iterations=50)
        
        # –†–∞–∑–º–µ—Ä—ã —É–∑–ª–æ–≤ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–≤—è–∑–µ–π
        node_sizes = []
        for node in G.nodes():
            total_connections = G.in_degree(node) + G.out_degree(node)
            node_sizes.append(max(300, total_connections * 100))
        
        # –†–∏—Å–æ–≤–∞–Ω–∏–µ
        nx.draw(G, pos, 
                node_size=node_sizes,
                node_color='lightblue',
                with_labels=True,
                font_size=8,
                font_weight='bold',
                arrows=True,
                arrowsize=20,
                edge_color='gray',
                alpha=0.7)
        
        plt.title('Hierarchy Graph Structure')
        plt.savefig(output_path / 'hierarchy_graph.png', dpi=300, bbox_inches='tight')
        plt.close()


def export_to_formats(pairs: List[Dict], terms: List[str], output_dir: str):
    """
    –≠–∫—Å–ø–æ—Ä—Ç –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
    
    Args:
        pairs: —Å–ø–∏—Å–æ–∫ –ø–∞—Ä
        terms: —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤
        output_dir: –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    output_path = Path(output_dir)
    
    # 1. CSV —Ñ–æ—Ä–º–∞—Ç
    df = pd.DataFrame(pairs)
    df.to_csv(output_path / 'predicted_pairs.csv', index=False, encoding='utf-8')
    
    # 2. TSV —Ñ–æ—Ä–º–∞—Ç
    df.to_csv(output_path / 'predicted_pairs.tsv', sep='\t', index=False, encoding='utf-8')
    
    # 3. GraphML —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Gephi/Cytoscape
    G = nx.DiGraph()
    G.add_nodes_from(terms)
    
    for pair in pairs:
        G.add_edge(pair['child'], pair['parent'])
    
    nx.write_graphml(G, output_path / 'hierarchy_graph.graphml')
    
    # 4. DOT —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Graphviz
    nx.write_dot(G, output_path / 'hierarchy_graph.dot')
    
    # 5. Adjacency matrix
    adj_matrix = nx.adjacency_matrix(G, nodelist=terms).toarray()
    np.savetxt(output_path / 'adjacency_matrix.csv', adj_matrix, delimiter=',', fmt='%d')
    
    # 6. Edge list
    with open(output_path / 'edge_list.txt', 'w', encoding='utf-8') as f:
        for pair in pairs:
            f.write(f"{pair['child']}\t{pair['parent']}\n")


def generate_report(analysis: Dict, metadata: Dict, output_dir: str):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á—ë—Ç–∞
    
    Args:
        analysis: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        metadata: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        output_dir: –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    output_path = Path(output_dir)
    
    report = f"""
# –û—Ç—á—ë—Ç –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ Cross-Attention –º–æ–¥–µ–ª–∏

## –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
- –î–∞—Ç–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {metadata['inference_info']['timestamp']}
- –†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü—ã: {metadata['inference_info']['matrix_shape']}
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥: {metadata['inference_info']['threshold_used']:.3f}

## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏
- ROC AUC: {metadata['model_info']['model_roc_auc']:.4f}
- –õ—É—á—à–∏–π F1 –ø–æ—Ä–æ–≥: {metadata['model_info']['best_f1_threshold']:.3f}
- –õ—É—á—à–∏–π F1 score: {metadata['model_info']['best_f1_value']:.4f}
- –î–∞—Ç–∞—Å–µ—Ç: {metadata['model_info']['dataset']}

## –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
- –í—Å–µ–≥–æ —Ç–µ—Ä–º–∏–Ω–æ–≤: {analysis['basic_stats']['total_terms']}
- –ù–∞–π–¥–µ–Ω–æ –ø–∞—Ä: {analysis['basic_stats']['total_pairs']}
- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–æ–¥–∏—Ç–µ–ª–µ–π: {analysis['basic_stats']['unique_parents']}
- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–µ—Ç–µ–π: {analysis['basic_stats']['unique_children']}
- –ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Å–≤—è–∑–µ–π: {analysis['basic_stats']['density']:.4f}

## –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —É–∑–ª–æ–≤
- –ö–æ—Ä–Ω–∏ (—Ç–æ–ª—å–∫–æ —Ä–æ–¥–∏—Ç–µ–ª–∏): {analysis['node_classification']['roots']}
- –õ–∏—Å—Ç—å—è (—Ç–æ–ª—å–∫–æ –¥–µ—Ç–∏): {analysis['node_classification']['leaves']}
- –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ (–∏ —Ä–æ–¥–∏—Ç–µ–ª–∏, –∏ –¥–µ—Ç–∏): {analysis['node_classification']['intermediate']}
- –ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ (–±–µ–∑ —Å–≤—è–∑–µ–π): {analysis['node_classification']['isolated']}

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏–µ—Ä–∞—Ä—Ö–∏–∏
- –°–≤—è–∑–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç: {analysis['connectivity']['connected_components']}
- –†–∞–∑–º–µ—Ä –Ω–∞–∏–±–æ–ª—å—à–µ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: {analysis['connectivity']['largest_component_size']}
- –î–æ–ª—è –≤ –Ω–∞–∏–±–æ–ª—å—à–µ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–µ: {analysis['connectivity']['largest_component_ratio']:.2%}
- –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: {analysis['connectivity']['average_component_size']:.1f}

## –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
- –ù–∞–π–¥–µ–Ω–æ —Ü–∏–∫–ª–æ–≤: {analysis['structure']['cycles_count']}
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞: {analysis['structure']['max_depth']}

## –¢–æ–ø —Ä–æ–¥–∏—Ç–µ–ª–µ–π (–ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –¥–µ—Ç–µ–π)
"""
    
    for parent, count in analysis['node_classification']['top_parents']:
        report += f"- {parent}: {count} –¥–µ—Ç–µ–π\n"
    
    report += "\n## –¢–æ–ø –¥–µ—Ç–µ–π (–ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ä–æ–¥–∏—Ç–µ–ª–µ–π)\n"
    
    for child, count in analysis['node_classification']['top_children']:
        report += f"- {child}: {count} —Ä–æ–¥–∏—Ç–µ–ª–µ–π\n"
    
    if analysis['structure']['cycles']:
        report += "\n## –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Ü–∏–∫–ª—ã\n"
        for i, cycle in enumerate(analysis['structure']['cycles'], 1):
            report += f"{i}. {' -> '.join(cycle)} -> {cycle[0]}\n"
    
    report += f"\n## –ü—Ä–∏–º–µ—Ä—ã –∫–æ—Ä–Ω–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤\n"
    for term in analysis['node_classification']['root_terms']:
        report += f"- {term}\n"
    
    report += f"\n## –ü—Ä–∏–º–µ—Ä—ã –ª–∏—Å—Ç–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤\n"
    for term in analysis['node_classification']['leaf_terms']:
        report += f"- {term}\n"
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞
    with open(output_path / 'analysis_report.md', 'w', encoding='utf-8') as f:
        f.write(report)


def analyze_results(
    results_dir: str,
    output_dir: str = None,
    create_visualizations: bool = False,
    export_formats: bool = False
) -> Dict:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Jupyter notebook
    
    Args:
        results_dir: Directory with inference results
        output_dir: Output directory for analysis (default: results_dir/analysis)
        create_visualizations: Create detailed visualizations
        export_formats: Export to multiple formats
        
    Returns:
        analysis_results: Dictionary with complete analysis results
    """
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–π –ø–∞–ø–∫–∏
    if output_dir is None:
        output_dir = f"{results_dir}/analysis"
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"üîç –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞...")
    print(f"üìÅ –ü–∞–ø–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: {results_dir}")
    print(f"üíæ –ü–∞–ø–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {output_dir}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("üìñ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    pred_matrix, pairs, terms, metadata = load_inference_results(results_dir)
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:")
    print(f"   –ú–∞—Ç—Ä–∏—Ü–∞: {pred_matrix.shape}")
    print(f"   –ü–∞—Ä—ã: {len(pairs)}")
    print(f"   –¢–µ—Ä–º–∏–Ω—ã: {len(terms)}")
    
    # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    print("üîç –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–µ—Ä–∞—Ä—Ö–∏–∏...")
    analysis = analyze_hierarchy_structure(pairs, terms)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
    with open(f"{output_dir}/analysis_results.json", 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞
    print("üìã –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞...")
    generate_report(analysis, metadata, output_dir)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
    if create_visualizations:
        print("üìä –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
        create_detailed_visualizations(pred_matrix, pairs, terms, metadata, output_dir)
    
    # –≠–∫—Å–ø–æ—Ä—Ç –≤ —Ñ–æ—Ä–º–∞—Ç—ã
    if export_formats:
        print("üíæ –≠–∫—Å–ø–æ—Ä—Ç –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã...")
        export_to_formats(pairs, terms, output_dir)
    
    print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_dir}")
    print(f"üìã –û—Ç—á—ë—Ç: {output_dir}/analysis_report.md")
    print(f"üìä –î–∞–Ω–Ω—ã–µ –∞–Ω–∞–ª–∏–∑–∞: {output_dir}/analysis_results.json")
    
    # –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞
    print("\nüìà –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞:")
    print(f"   –¢–µ—Ä–º–∏–Ω–æ–≤: {analysis['basic_stats']['total_terms']}")
    print(f"   –ü–∞—Ä: {analysis['basic_stats']['total_pairs']}")
    print(f"   –ö–æ—Ä–Ω–∏: {analysis['node_classification']['roots']}")
    print(f"   –õ–∏—Å—Ç—å—è: {analysis['node_classification']['leaves']}")
    print(f"   –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: {analysis['connectivity']['connected_components']}")
    print(f"   –¶–∏–∫–ª—ã: {analysis['structure']['cycles_count']}")
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    return {
        'pred_matrix': pred_matrix,
        'pairs': pairs,
        'terms': terms,
        'metadata': metadata,
        'analysis': analysis,
        'output_dir': output_dir,
        'files_created': {
            'analysis_json': f"{output_dir}/analysis_results.json",
            'report_md': f"{output_dir}/analysis_report.md",
            'visualizations': f"{output_dir}/*.png" if create_visualizations else None,
            'export_formats': [
                f"{output_dir}/predicted_pairs.csv",
                f"{output_dir}/predicted_pairs.tsv",
                f"{output_dir}/hierarchy_graph.graphml",
                f"{output_dir}/hierarchy_graph.dot",
                f"{output_dir}/adjacency_matrix.csv",
                f"{output_dir}/edge_list.txt"
            ] if export_formats else None
        }
    }


def main():
    parser = argparse.ArgumentParser(description="Analyze Cross-Attention Inference Results")
    parser.add_argument("--results_dir", type=str, required=True,
                        help="Directory with inference results")
    parser.add_argument("--output_dir", type=str, default=None,
                        help="Output directory for analysis (default: results_dir/analysis)")
    parser.add_argument("--create_visualizations", action="store_true",
                        help="Create detailed visualizations")
    parser.add_argument("--export_formats", action="store_true",
                        help="Export to multiple formats")
    
    args = parser.parse_args()
    
    # –í—ã–∑–æ–≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
    results = analyze_results(
        results_dir=args.results_dir,
        output_dir=args.output_dir,
        create_visualizations=args.create_visualizations,
        export_formats=args.export_formats
    )
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö
    print(f"\nüìã –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    print(f"   üìä JSON —Å –∞–Ω–∞–ª–∏–∑–æ–º: {results['files_created']['analysis_json']}")
    print(f"   üìù –û—Ç—á—ë—Ç: {results['files_created']['report_md']}")
    
    if results['files_created']['visualizations']:
        print(f"   üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {results['files_created']['visualizations']}")
    
    if results['files_created']['export_formats']:
        print(f"   üíæ –≠–∫—Å–ø–æ—Ä—Ç —Ñ–æ—Ä–º–∞—Ç—ã: {len(results['files_created']['export_formats'])} —Ñ–∞–π–ª–æ–≤")
    
    return results


if __name__ == "__main__":
    main() 