#!/usr/bin/env python3
"""
Script for evaluating the best models from all experiments
Loads the best models, recreates test data and performs threshold analysis
"""

import os
import json
import re
import torch
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import argparse
from datetime import datetime
import logging

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
from cross_attention_model import CrossAttentionModel
from dataset import create_train_test_datasets
from train_cross_attention import (
    evaluate_model, 
    analyze_thresholds, 
    save_metrics
)


def setup_logging() -> logging.Logger:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
    logger = logging.getLogger("evaluate_best_models")
    logger.setLevel(logging.INFO)
    
    # –û—á–∏—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ handlers
    logger.handlers.clear()
    
    # –§–æ—Ä–º–∞—Ç—Ç–µ—Ä –¥–ª—è –ª–æ–≥–æ–≤
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Handler –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(console_handler)
    
    return logger


def parse_experiment_name(experiment_name: str) -> Dict[str, Any]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –∏–º–µ–Ω–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    
    Args:
        experiment_name: –∏–º—è –ø–∞–ø–∫–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        
    Returns:
        dict —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    """
    # –£–±–∏—Ä–∞–µ–º timestamp –≤ –Ω–∞—á–∞–ª–µ (–¥–∞—Ç–∞ –∏ –≤—Ä–µ–º—è: 20250708_134939_...)
    parts = experiment_name.split('_')
    
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ –¥–≤–µ —á–∞—Å—Ç–∏ –µ—Å–ª–∏ –æ–Ω–∏ –≤—ã–≥–ª—è–¥—è—Ç –∫–∞–∫ –¥–∞—Ç–∞ –∏ –≤—Ä–µ–º—è
    start_idx = 0
    if len(parts) >= 2 and parts[0].isdigit() and len(parts[0]) == 8 and parts[1].isdigit():
        start_idx = 2  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–∞—Ç—É –∏ –≤—Ä–µ–º—è
    elif len(parts) >= 1 and parts[0].isdigit():
        start_idx = 1  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å
    
    parts = parts[start_idx:]
    
    params = {}
    
    for i, part in enumerate(parts):
        if part.startswith('ep'):
            params['epochs'] = int(part[2:])
        elif part.startswith('lr'):
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–∞—É—á–Ω—É—é –Ω–æ—Ç–∞—Ü–∏—é
            lr_str = part[2:]
            if 'e-' in lr_str:
                params['lr'] = float(lr_str)
            else:
                params['lr'] = float(lr_str)
        elif part.startswith('bs'):
            params['batch_size'] = int(part[2:])
        elif part.startswith('eval'):
            params['eval_every'] = int(part[4:])
        elif part.startswith('seed'):
            params['seed'] = int(part[4:])
        elif part == 'qwen3':
            params['use_qwen3'] = True
        elif part.startswith('ds_'):
            params['dataset_strategy'] = part[3:]
        elif part.startswith('samp_'):
            params['sampling_strategy'] = part[5:]
        elif part.startswith('pos'):
            params['positive_ratio'] = float(part[3:])
        elif part.startswith('max'):
            params['max_steps'] = int(part[3:])
        elif i == 0:  # –ü–µ—Ä–≤–∞—è —á–∞—Å—Ç—å –ø–æ—Å–ª–µ timestamp - —ç—Ç–æ dataset
            params['dataset_name'] = part
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    params.setdefault('use_qwen3', False)
    params.setdefault('dataset_strategy', 'single')
    params.setdefault('sampling_strategy', 'balanced')
    params.setdefault('positive_ratio', 1.0)
    params.setdefault('max_steps', None)
    params.setdefault('test_size', 0.2)
    
    return params


def find_best_model(experiment_path: str) -> Optional[Dict[str, Any]]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –≤ –ø–∞–ø–∫–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    
    Args:
        experiment_path: –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        
    Returns:
        dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ None
    """
    best_models_path = os.path.join(experiment_path, 'best_models')
    best_models_json = os.path.join(best_models_path, 'best_models.json')
    
    if not os.path.exists(best_models_json):
        return None
    
    try:
        with open(best_models_json, 'r') as f:
            data = json.load(f)
        
        if not data.get('best_models'):
            return None
            
        # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å - –ø–µ—Ä–≤–∞—è –≤ —Å–ø–∏—Å–∫–µ (—Å –Ω–∞–∏–≤—ã—Å—à–∏–º ROC AUC)
        best_model = data['best_models'][0]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏
        model_path = best_model['model_path']
        if not os.path.exists(model_path):
            return None
            
        return best_model
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {best_models_json}: {e}")
        return None


def get_dataset_paths(dataset_name: str) -> Tuple[str, str]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º –¥–∞—Ç–∞—Å–µ—Ç–∞
    
    Args:
        dataset_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
        
    Returns:
        tuple —Å –ø—É—Ç—è–º–∏ –∫ entities –∏ relations
    """
    # –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–∞–Ω–Ω—ã–º–∏
    data_dir = "/home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/2025/TaskC-TaxonomyDiscovery"
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞: DatasetName/train/datasetname_train_types_embeddings.json –∏ datasetname_train_pairs.json
    dataset_train_dir = os.path.join(data_dir, dataset_name, "train")
    
    # –ò–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—é—Ç lowercase –∏–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset_lower = dataset_name.lower()
    
    entities_path = os.path.join(dataset_train_dir, f"{dataset_lower}_train_types_embeddings.json")
    relations_path = os.path.join(dataset_train_dir, f"{dataset_lower}_train_pairs.json")
    
    return entities_path, relations_path


def load_model(model_path: str, use_qwen3: bool = False) -> CrossAttentionModel:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ —Ñ–∞–π–ª–∞
    
    Args:
        model_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏
        use_qwen3: –±—ã–ª–∞ –ª–∏ –º–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∏–∑ Qwen3 (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
        
    Returns:
        –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º checkpoint
    checkpoint = torch.load(model_path, map_location='cpu')
    
    # –ü–æ–ª—É—á–∞–µ–º config –∏–∑ checkpoint –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π
    if 'config' in checkpoint and not use_qwen3:
        # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ–º config –∏–∑ checkpoint
        config = checkpoint['config']
    else:
        # –ó–∞—Ö–∞—Ä–¥–∫–æ–∂–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ checkpoint'–∞–º–∏
        if use_qwen3:
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Qwen3 –º–æ–¥–µ–ª–µ–π (–∏–∑ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –≤–µ—Å–æ–≤)
            config = {
                'hidden_size': 2560,
                'num_attention_heads': 32,  # 4096 / 128
                'num_key_value_heads': 8,   # 1024 / 128  
                'head_dim': 128,            # q_norm.weight.shape[0]
                'rms_norm_eps': 1e-6,
                'attention_bias': False
            }
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            config = {
                'hidden_size': 2560,
                'num_attention_heads': 8,
                'num_key_value_heads': 8,
                'head_dim': 320,  # 2560 / 8
                'rms_norm_eps': 1e-6,
                'attention_bias': False
            }
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º config
    model = CrossAttentionModel(config)
    
    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    if 'layer_idx' in checkpoint:
        model.layer_idx = checkpoint['layer_idx']
    if 'initialized_from_qwen3' in checkpoint:
        model.initialized_from_qwen3 = checkpoint['initialized_from_qwen3']
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    if 'model_state_dict' in checkpoint:
        # –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø–æ–ª—è–º–∏
        model.load_state_dict(checkpoint['model_state_dict'])
    elif isinstance(checkpoint, dict) and 'q_proj.weight' in checkpoint:
        # –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ –ø—Ä–æ—Å—Ç–æ–π state_dict
        model.load_state_dict(checkpoint)
    else:
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å
        model.load_state_dict(checkpoint)
    
    return model


def evaluate_experiment(experiment_path: str, logger: logging.Logger) -> bool:
    """
    –û—Ü–µ–Ω–∫–∞ –æ–¥–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    
    Args:
        experiment_path: –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        logger: –ª–æ–≥–≥–µ—Ä
        
    Returns:
        True –µ—Å–ª–∏ –æ—Ü–µ–Ω–∫–∞ –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ
    """
    experiment_name = os.path.basename(experiment_path)
    logger.info(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {experiment_name}")
    
    # –ü–∞—Ä—Å–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    try:
        params = parse_experiment_name(experiment_name)
        logger.info(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {params}")
    except Exception as e:
        logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏–º–µ–Ω–∏: {e}")
        return False
    
    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    best_model_info = find_best_model(experiment_path)
    if not best_model_info:
        logger.warning(f"   ‚ö†Ô∏è –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return False
        
    logger.info(f"   üìä –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: ROC AUC = {best_model_info['roc_auc']:.4f}, Step = {best_model_info['step']}")
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
    try:
        entities_path, relations_path = get_dataset_paths(params['dataset_name'])
        if not os.path.exists(entities_path) or not os.path.exists(relations_path):
            logger.error(f"   ‚ùå –§–∞–π–ª—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {entities_path}, {relations_path}")
            return False
    except Exception as e:
        logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—É—Ç–µ–π –∫ –¥–∞–Ω–Ω—ã–º: {e}")
        return False
    
    # –í–æ—Å—Å–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã —Å —Ç–µ–º –∂–µ seed
    try:
        logger.info(f"   üîÑ –í–æ—Å—Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ —Å seed={params['seed']}")
        train_dataset, test_dataset = create_train_test_datasets(
            entities_path,
            relations_path,
            batch_size_1=params['batch_size'],
            batch_size_2=params['batch_size'],
            dataset_strategy=params['dataset_strategy'],
            sampling_strategy=params['sampling_strategy'],
            positive_ratio=params['positive_ratio'],
            test_part=params['test_size'],
            random_state=params['seed']
        )
        logger.info(f"   ‚úÖ –î–∞—Ç–∞—Å–µ—Ç—ã —Å–æ–∑–¥–∞–Ω—ã: Train={len(train_dataset)}, Test={len(test_dataset)}")
    except Exception as e:
        logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {e}")
        return False
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    try:
        logger.info(f"   üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {best_model_info['model_path']}")
        model = load_model(best_model_info['model_path'], params['use_qwen3'])
        logger.info(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    except Exception as e:
        logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return False
    
    # –°–æ–∑–¥–∞–µ–º DataLoader –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    from torch.utils.data import DataLoader
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=1,
        shuffle=False,
        num_workers=0,
        pin_memory=True if device == "cuda" else False
    )
    
    # –ü—Ä–æ–≤–æ–¥–∏–º –æ—Ü–µ–Ω–∫—É
    try:
        logger.info(f"   üéØ –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        final_metrics = evaluate_model(model, test_loader, device)
        logger.info(f"   üìä ROC AUC: {final_metrics['roc_auc']:.4f}")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ—Ä–æ–≥–æ–≤
        logger.info(f"   üîç –ê–Ω–∞–ª–∏–∑ –ø–æ—Ä–æ–≥–æ–≤")
        threshold_analysis = analyze_thresholds(model, test_loader, device, None)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–ø–∫—É best_models
        best_models_dir = os.path.join(experiment_path, 'best_models')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        final_metrics_path = os.path.join(best_models_dir, 'best_model_final_metrics.json')
        save_metrics(final_metrics, final_metrics_path)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –ø–æ—Ä–æ–≥–æ–≤
        threshold_analysis_path = os.path.join(best_models_dir, 'best_model_threshold_analysis.json')
        save_metrics(threshold_analysis, threshold_analysis_path)
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–∫—É
        summary = {
            'experiment_name': experiment_name,
            'experiment_params': params,
            'best_model_info': best_model_info,
            'evaluation_timestamp': datetime.now().isoformat(),
            'final_metrics': final_metrics,
            'best_thresholds': threshold_analysis['best_thresholds'],
            'best_values': threshold_analysis['best_values']
        }
        
        summary_path = os.path.join(best_models_dir, 'best_model_evaluation_summary.json')
        save_metrics(summary, summary_path)
        
        logger.info(f"   ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {best_models_dir}")
        logger.info(f"   üéØ –õ—É—á—à–∏–µ –ø–æ—Ä–æ–≥–∏:")
        for metric, threshold in threshold_analysis['best_thresholds'].items():
            value = threshold_analysis['best_values'][metric]
            logger.info(f"      {metric.capitalize()}: {threshold:.2f} (–∑–Ω–∞—á–µ–Ω–∏–µ: {value:.4f})")
        
        return True
        
    except Exception as e:
        logger.error(f"   ‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏: {e}")
        return False


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description="–û—Ü–µ–Ω–∫–∞ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")
    parser.add_argument("--results_dir", type=str, 
                       default="/home/jovyan/rahmatullaev/rand_exps/LLMs4OL-Challenge/src/taskC/method_v5_hm/results",
                       help="–ü–∞–ø–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")
    parser.add_argument("--experiment_filter", type=str, default=None,
                       help="–§–∏–ª—å—Ç—Ä –¥–ª—è –∏–º–µ–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ (—Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ)")
    parser.add_argument("--max_experiments", type=int, default=None,
                       help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger = setup_logging()
    
    logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –æ—Ü–µ–Ω–∫–∏ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π")
    logger.info(f"üìÅ –ü–∞–ø–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: {args.results_dir}")
    
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –ø–∞–ø–∫–∏ —Å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏
    experiment_dirs = []
    for item in os.listdir(args.results_dir):
        item_path = os.path.join(args.results_dir, item)
        if os.path.isdir(item_path) and not item.startswith('.'):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞–ø–∫–∏ grid_search_*
            if item.startswith('grid_search_'):
                continue
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
            if args.experiment_filter:
                if not re.search(args.experiment_filter, item):
                    continue
            experiment_dirs.append(item_path)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–º–µ–Ω–∏
    experiment_dirs.sort()
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
    if args.max_experiments:
        experiment_dirs = experiment_dirs[:args.max_experiments]
    
    logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {len(experiment_dirs)}")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
    success_count = 0
    error_count = 0
    
    for i, experiment_path in enumerate(experiment_dirs, 1):
        logger.info(f"\nüìç –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(experiment_dirs)}")
        
        try:
            if evaluate_experiment(experiment_path, logger):
                success_count += 1
            else:
                error_count += 1
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {experiment_path}: {e}")
            error_count += 1
    
    logger.info(f"\nüèÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    logger.info(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {success_count}")
    logger.info(f"   ‚ùå –û—à–∏–±–æ–∫: {error_count}")
    logger.info(f"   üìä –í—Å–µ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {len(experiment_dirs)}")


if __name__ == "__main__":
    main() 